#!/usr/bin/env python3
"""
RAG Core - Multimodal & Text-only RAG Classes

ì´ ëª¨ë“ˆì€ Milvus ê¸°ë°˜ RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ í´ë˜ìŠ¤ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:
- MultimodalRAGV2: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë©€í‹°ëª¨ë‹¬ RAG
- MilvusLiteRAGV2: í…ìŠ¤íŠ¸ ì „ìš© RAG (Milvus Lite)

ì‚¬ìš©ë²•:
    from rag_core import MultimodalRAGV2, MilvusLiteRAGV2
    
    # Multimodal RAG
    rag = MultimodalRAGV2(db_file="./multimodal.db")
    results = rag.search_by_text("í™”ì¬ ì‚¬ê±´")
    
    # Text-only RAG
    rag = MilvusLiteRAGV2(collection_name="fire_news", db_file="./fire.db")
    results = rag.search_by_text("í™”ì¬ ì‚¬ê±´")
"""

import argparse
import logging
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import numpy as np

# Milvus Client (High-level API)
from pymilvus import MilvusClient

# Embeddings
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# CLIP for image embeddings
try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    from PIL import Image
    import requests
    from io import BytesIO
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    logging.warning("transformers not installed. Install with: pip install transformers torch pillow")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MultimodalRAGV2:
    def __init__(
        self,
        db_file: str = "./multimodal_rag.db",
        text_model: str = "jhgan/ko-sroberta-multitask",
        clip_model: str = "openai/clip-vit-base-patch32",
    ):
        """
        Multimodal RAG ì‹œìŠ¤í…œ (MilvusClient API ì‚¬ìš©)
        
        Args:
            db_file: Milvus Lite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼
            text_model: í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
            clip_model: ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸ (CLIP)
        """
        self.db_file = db_file
        
        # Milvus Client ì´ˆê¸°í™”
        self.client = MilvusClient(db_file)
        logger.info(f"âœ… Connected to Milvus at {db_file}")
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
        logger.info(f"Loading text embedding model: {text_model}")
        self.text_embeddings = HuggingFaceEmbeddings(
            model_name=text_model,
            model_kwargs={'device': 'cuda' if self._check_cuda() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.text_dim = 768  # ko-sroberta dimension
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸ (CLIP)
        self.clip_model = None
        self.clip_processor = None
        self.image_dim = 512  # CLIP dimension
        
        if CLIP_AVAILABLE:
            logger.info(f"Loading CLIP model: {clip_model}")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.clip_model = CLIPModel.from_pretrained(clip_model).to(self.device)
            self.clip_processor = CLIPProcessor.from_pretrained(clip_model)
            logger.info(f"âœ… CLIP model loaded on {self.device}")
        else:
            logger.warning("CLIP not available. Image search will be disabled.")

    def _check_cuda(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False

    def create_collection(self, collection_name: str, has_image: bool = True):
        """
        Multimodal ì»¬ë ‰ì…˜ ìƒì„±
        
        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            has_image: ì´ë¯¸ì§€ ë²¡í„° í¬í•¨ ì—¬ë¶€
        """
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if self.client.has_collection(collection_name):
            logger.info(f"Dropping existing collection '{collection_name}'...")
            self.client.drop_collection(collection_name)
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        # ì°¸ê³ : MilvusClientëŠ” ë‹¨ì¼ ë²¡í„° í•„ë“œë§Œ ì§€ì›í•˜ë¯€ë¡œ,
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë²¡í„°ë¥¼ concatí•˜ê±°ë‚˜ ë³„ë„ ì»¬ë ‰ì…˜ ì‚¬ìš©
        dimension = self.text_dim + self.image_dim if has_image else self.text_dim
        
        self.client.create_collection(
            collection_name=collection_name,
            dimension=dimension,
            metric_type="COSINE",
            auto_id=True,
            enable_dynamic_field=True,  # ì¶”ê°€ í•„ë“œ í—ˆìš©
        )
        
        logger.info(f"âœ… Created collection '{collection_name}' with dimension={dimension}")

    def load_image(self, image_source: str) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ë¡œë“œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” URL)"""
        try:
            if image_source.startswith('http://') or image_source.startswith('https://'):
                response = requests.get(image_source, timeout=10)
                image = Image.open(BytesIO(response.content)).convert('RGB')
            else:
                image = Image.open(image_source).convert('RGB')
            return image
        except Exception as e:
            logger.warning(f"Failed to load image from {image_source}: {e}")
            return None

    def encode_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        return self.text_embeddings.embed_query(text)

    def encode_image(self, image: Image.Image) -> Optional[List[float]]:
        """ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (CLIP)"""
        if not CLIP_AVAILABLE or self.clip_model is None:
            return None
        
        try:
            inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            return image_features.cpu().numpy().flatten().tolist()
        except Exception as e:
            logger.error(f"Failed to encode image: {e}")
            return None

    def prepare_multimodal_data(
        self,
        news_data: List[Dict[str, Any]],
        image_mappings: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Multimodal í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
        
        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            image_mappings: {doc_id: image_path} ë§¤í•‘
        
        Returns:
            Milvus ì‚½ì…ìš© ë°ì´í„°
        """
        logger.info(f"Preparing {len(news_data)} items for multimodal storage...")
        
        milvus_data = []
        
        for idx, news in enumerate(tqdm(news_data, desc="Processing items")):
            doc_id = news.get("doc_id", f"doc_{idx}")
            title = news.get("title", "")
            content = news.get("text", news.get("content", ""))
            
            if len(content.strip()) < 50:
                continue
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            text_embedding = self.encode_text(content)
            
            # ì´ë¯¸ì§€ ì„ë² ë”© (ìˆëŠ” ê²½ìš°)
            image_embedding = None
            image_path = ""
            has_image = False
            
            if image_mappings and doc_id in image_mappings:
                image_path = image_mappings[doc_id]
                image = self.load_image(image_path)
                if image:
                    image_embedding = self.encode_image(image)
                    if image_embedding:
                        has_image = True
            
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë²¡í„° ê²°í•©
            if has_image and image_embedding:
                # Concat: [text_vec, image_vec]
                combined_vector = text_embedding + image_embedding
            else:
                # ì´ë¯¸ì§€ ì—†ìœ¼ë©´ zero padding
                combined_vector = text_embedding + [0.0] * self.image_dim
            
            # ë°ì´í„° í•­ëª©
            data_item = {
                "vector": combined_vector,
                "doc_id": doc_id,
                "title": title[:500] if title else "",
                "content": content[:5000] if content else "",
                "date": news.get("date", "")[:20],
                "url": news.get("url", "")[:500],
                "source": news.get("source", "")[:200],
                "category": news.get("category", "")[:50],
                "topic": news.get("topic", "")[:100],
                "has_image": has_image,
                "image_path": image_path[:500] if image_path else "",
            }
            
            milvus_data.append(data_item)
        
        logger.info(f"Prepared {len(milvus_data)} items")
        return milvus_data

    def insert_data(self, collection_name: str, data: List[Dict[str, Any]]) -> bool:
        """ë°ì´í„° ì‚½ì…"""
        try:
            logger.info(f"Inserting {len(data)} items...")
            
            # ë°°ì¹˜ ì‚½ì…
            batch_size = 1000
            for i in tqdm(range(0, len(data), batch_size), desc="Inserting"):
                batch = data[i:i+batch_size]
                self.client.insert(collection_name=collection_name, data=batch)
            
            logger.info(f"âœ… Successfully inserted {len(data)} items")
            return True
        except Exception as e:
            logger.error(f"Failed to insert data: {e}")
            import traceback
            traceback.print_exc()
            return False

    def search_by_text(
        self,
        collection_name: str,
        query: str,
        top_k: int = 5,
        filter_expr: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰"""
        logger.info(f"Searching with text: '{query}'")
        
        # ì¿¼ë¦¬ ë²¡í„° (í…ìŠ¤íŠ¸ë§Œ)
        text_vec = self.encode_text(query)
        query_vec = text_vec + [0.0] * self.image_dim  # Zero padding for image part
        
        results = self.client.search(
            collection_name=collection_name,
            data=[query_vec],
            limit=top_k,
            filter=filter_expr,
            output_fields=["doc_id", "title", "content", "date", "category", "has_image", "image_path"]
        )
        
        return self._format_results(results)

    def search_by_image(
        self,
        collection_name: str,
        image_path: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰"""
        if not CLIP_AVAILABLE:
            logger.error("CLIP not available for image search")
            return []
        
        logger.info(f"Searching with image: {image_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì¸ì½”ë”©
        image = self.load_image(image_path)
        if not image:
            return []
        
        image_vec = self.encode_image(image)
        if not image_vec:
            return []
        
        # ì¿¼ë¦¬ ë²¡í„° (ì´ë¯¸ì§€ë§Œ)
        query_vec = [0.0] * self.text_dim + image_vec  # Zero padding for text part
        
        results = self.client.search(
            collection_name=collection_name,
            data=[query_vec],
            limit=top_k,
            output_fields=["doc_id", "title", "content", "has_image", "image_path"]
        )
        
        return self._format_results(results)

    def hybrid_search(
        self,
        collection_name: str,
        query_text: Optional[str] = None,
        query_image: Optional[str] = None,
        text_weight: float = 0.5,
        image_weight: float = 0.5,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)"""
        logger.info("Performing hybrid search...")
        
        text_vec = [0.0] * self.text_dim
        image_vec = [0.0] * self.image_dim
        
        # í…ìŠ¤íŠ¸ ë²¡í„°
        if query_text:
            text_vec_raw = self.encode_text(query_text)
            text_vec = [v * text_weight for v in text_vec_raw]
        
        # ì´ë¯¸ì§€ ë²¡í„°
        if query_image and CLIP_AVAILABLE:
            image = self.load_image(query_image)
            if image:
                image_vec_raw = self.encode_image(image)
                if image_vec_raw:
                    image_vec = [v * image_weight for v in image_vec_raw]
        
        # ê²°í•©
        query_vec = text_vec + image_vec
        
        results = self.client.search(
            collection_name=collection_name,
            data=[query_vec],
            limit=top_k,
            output_fields=["doc_id", "title", "content", "date", "has_image", "image_path"]
        )
        
        return self._format_results(results)

    def _format_results(self, results) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for hits in results:
            for hit in hits:
                result = {
                    "id": hit.get("id"),
                    "distance": hit.get("distance"),
                }
                result.update(hit.get("entity", {}))
                formatted.append(result)
        return formatted

    def print_results(self, results: List[Dict[str, Any]]):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print(f"ğŸ” Multimodal Search Results ({len(results)} items)")
        print("="*80)
        
        for idx, result in enumerate(results, 1):
            print(f"\nğŸ“„ Result #{idx}")
            print(f"   Distance: {result.get('distance', 0):.4f}")
            print(f"   Title: {result.get('title', 'N/A')}")
            print(f"   Date: {result.get('date', 'N/A')}")
            print(f"   Category: {result.get('category', 'N/A')}")
            
            if result.get('has_image'):
                print(f"   ğŸ–¼ï¸ Image: {result.get('image_path', 'N/A')}")
            
            content = result.get('content', '')
            if content:
                preview = content[:200] + "..." if len(content) > 200 else content
                print(f"   Content: {preview}")
            
            print("-" * 80)


def main():
    parser = argparse.ArgumentParser(description='Multimodal RAG V2 with MilvusClient')
    parser.add_argument('--mode', required=True,
                       choices=['create', 'search', 'search-image', 'hybrid'],
                       help='Operation mode')
    parser.add_argument('--collection', '-c', required=True,
                       help='Collection name')
    parser.add_argument('--input', '-i',
                       help='Input JSON file (for create mode)')
    parser.add_argument('--images',
                       help='Image mappings JSON file')
    parser.add_argument('--query', '-q',
                       help='Text query (for search modes)')
    parser.add_argument('--image', '-img',
                       help='Image path (for image search)')
    parser.add_argument('--top-k', '-k', type=int, default=5,
                       help='Number of results')
    parser.add_argument('--db-file', default='./multimodal_rag.db',
                       help='Milvus database file')
    
    args = parser.parse_args()
    
    # ì´ˆê¸°í™”
    rag = MultimodalRAGV2(db_file=args.db_file)
    
    if args.mode == 'create':
        # ë°ì´í„° ë¡œë“œ
        if not args.input:
            logger.error("--input required for create mode")
            return
        
        with open(args.input, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        
        # ì´ë¯¸ì§€ ë§¤í•‘ ë¡œë“œ (ì„ íƒì )
        image_mappings = None
        if args.images:
            with open(args.images, 'r', encoding='utf-8') as f:
                image_list = json.load(f)
                image_mappings = {item['doc_id']: item['image_path'] for item in image_list}
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        rag.create_collection(args.collection, has_image=True)
        
        # ë°ì´í„° ì¤€ë¹„ ë° ì‚½ì…
        data = rag.prepare_multimodal_data(news_data, image_mappings)
        rag.insert_data(args.collection, data)
        
    elif args.mode == 'search':
        if not args.query:
            logger.error("--query required for search mode")
            return
        
        results = rag.search_by_text(args.collection, args.query, top_k=args.top_k)
        rag.print_results(results)
        
    elif args.mode == 'search-image':
        if not args.image:
            logger.error("--image required for search-image mode")
            return
        
        results = rag.search_by_image(args.collection, args.image, top_k=args.top_k)
        rag.print_results(results)
        
    elif args.mode == 'hybrid':
        results = rag.hybrid_search(
            args.collection,
            query_text=args.query,
            query_image=args.image,
            top_k=args.top_k
        )
        rag.print_results(results)


class MilvusLiteRAGV2:
    """Milvus Lite RAG ê²€ìƒ‰ í´ë˜ìŠ¤ (í…ìŠ¤íŠ¸ ì „ìš©)"""
    
    def __init__(
        self,
        collection_name: str,
        db_file: str = "./milvus_lite_v2.db",
        text_model: str = "jhgan/ko-sroberta-multitask"
    ):
        """
        Milvus Lite RAG ê²€ìƒ‰ í´ë˜ìŠ¤ V2 (ê³µì‹ API ì‚¬ìš©)
        
        Args:
            collection_name: ê²€ìƒ‰í•  Milvus ì»¬ë ‰ì…˜ ì´ë¦„
            db_file: Milvus Lite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
            text_model: í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
        """
        self.collection_name = collection_name
        self.db_file = db_file
        
        # Milvus Client ì´ˆê¸°í™”
        self.client = MilvusClient(db_file)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        if not self.client.has_collection(collection_name):
            raise ValueError(f"Collection '{collection_name}' does not exist in {db_file}")
        
        # ì»¬ë ‰ì…˜ í†µê³„
        stats = self.client.get_collection_stats(collection_name)
        logger.info(f"âœ… Connected to collection '{collection_name}'")
        logger.info(f"ğŸ“Š Collection stats: {stats}")
        logger.info(f"ğŸ“ Database: {db_file}")
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
        logger.info(f"Loading text embedding model: {text_model}")
        device = 'cuda' if self._check_cuda() else 'cpu'
        self.text_embeddings = HuggingFaceEmbeddings(
            model_name=text_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )

    def _check_cuda(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False

    def search_by_text(
        self,
        query: str,
        top_k: int = 5,
        filter_expr: Optional[str] = None,
        output_fields: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filter_expr: í•„í„° í‘œí˜„ì‹ (ì˜ˆ: 'category == "disaster"')
            output_fields: ë°˜í™˜í•  í•„ë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"Searching with text query: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.text_embeddings.embed_query(query)
        
        # ê¸°ë³¸ ì¶œë ¥ í•„ë“œ
        if output_fields is None:
            output_fields = ["doc_id", "title", "content", "date", "category", "topic", "url", "source"]
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = self.client.search(
            collection_name=self.collection_name,
            data=[query_embedding],
            limit=top_k,
            filter=filter_expr,
            output_fields=output_fields,
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for hits in results:
            for hit in hits:
                result = {
                    "id": hit.get("id"),
                    "distance": hit.get("distance"),
                    "entity": hit.get("entity", {})
                }
                result.update(result["entity"])
                formatted_results.append(result)
        
        logger.info(f"Found {len(formatted_results)} results")
        return formatted_results

    def search_with_metadata_filter(
        self,
        query: str,
        category: Optional[str] = None,
        topic: Optional[str] = None,
        date_start: Optional[str] = None,
        date_end: Optional[str] = None,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            category: ì¹´í…Œê³ ë¦¬ í•„í„°
            topic: í† í”½ í•„í„°
            date_start: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            date_end: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # í•„í„° í‘œí˜„ì‹ êµ¬ì„±
        filter_parts = []
        
        if category:
            filter_parts.append(f'category == "{category}"')
        
        if topic:
            filter_parts.append(f'topic == "{topic}"')
        
        if date_start:
            filter_parts.append(f'date >= "{date_start}"')
        
        if date_end:
            filter_parts.append(f'date <= "{date_end}"')
        
        filter_expr = " && ".join(filter_parts) if filter_parts else None
        
        if filter_expr:
            logger.info(f"Searching with filter: {filter_expr}")
        
        return self.search_by_text(query, top_k=top_k, filter_expr=filter_expr)

    def print_results(self, results: List[Dict[str, Any]], show_content: bool = False):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*80)
        print(f"ğŸ” Search Results ({len(results)} items)")
        print("="*80)
        
        for idx, result in enumerate(results, 1):
            print(f"\nğŸ“„ Result #{idx}")
            print(f"   Distance: {result.get('distance', 0):.4f}")
            print(f"   ID: {result.get('id', 'N/A')}")
            print(f"   Title: {result.get('title', 'N/A')}")
            print(f"   Date: {result.get('date', 'N/A')}")
            print(f"   Category: {result.get('category', 'N/A')} | Topic: {result.get('topic', 'N/A')}")
            print(f"   Source: {result.get('source', 'N/A')}")
            
            if result.get('url'):
                print(f"   URL: {result['url']}")
            
            if show_content and result.get('content'):
                content = result['content'][:200] + "..." if len(result['content']) > 200 else result['content']
                print(f"   Content: {content}")
            
            print("-" * 80)


class AnswerGenerator:
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±ê¸° (OpenAI/Anthropic)"""
    
    def __init__(self, llm: str = "openai", model: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            llm: LLM ì œê³µì ("openai" ë˜ëŠ” "anthropic")
            model: ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: gpt-4o-mini / claude-sonnet-4-20250514)
        """
        self.llm = llm
        if llm == "openai":
            from openai import OpenAI
            self.client = OpenAI()
            self.model = model or "gpt-4o-mini"
            logger.info(f"âœ… OpenAI client initialized with model: {self.model}")
        else:
            import anthropic
            self.client = anthropic.Anthropic()
            self.model = model or "claude-sonnet-4-20250514"
            logger.info(f"âœ… Anthropic client initialized with model: {self.model}")
    
    def generate(self, query: str, results: List[Dict[str, Any]], max_context_chars: int = 8000) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_context_chars: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        context = self._build_context(results, max_context_chars)
        prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ì§ˆë¬¸: {query}

ê²€ìƒ‰ ê²°ê³¼:
{context}

ë‹µë³€:"""
        
        logger.info(f"Generating answer with {self.llm} ({self.model})")
        
        if self.llm == "openai":
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            return resp.choices[0].message.content
        else:
            resp = self.client.messages.create(
                model=self.model,
                max_tokens=2048,
                messages=[{"role": "user", "content": prompt}]
            )
            return resp.content[0].text
    
    def _build_context(self, results: List[Dict[str, Any]], max_chars: int) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        chunks = []
        total = 0
        for i, r in enumerate(results):
            entity = r.get('entity', {})
            title = entity.get('title', 'N/A')
            content = entity.get('content', '')[:2000]
            source = entity.get('source', '')
            coll = r.get('_collection', 'unknown')
            chunk = f"[{i+1}] {title}\nì¶œì²˜: {source} (Collection: {coll})\n{content}"
            if total + len(chunk) > max_chars:
                break
            chunks.append(chunk)
            total += len(chunk)
        return "\n\n".join(chunks)


if __name__ == "__main__":
    main()

