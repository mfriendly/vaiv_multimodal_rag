#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë°ì´í„° â†’ Milvus Lite ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ V2 (ê³µì‹ API ì‚¬ìš©)

ì‚¬ìš©ë²•:
    python convert_news_to_milvus_lite_v2.py --input news_data/01_disaster_Fire_3years.json --collection fire_news
"""

import json
import os
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import time
from tqdm import tqdm
import logging

# Milvus Client (High-level API)
from pymilvus import MilvusClient

# LangChain imports
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# OpenAI for metadata extraction
import openai

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class NewsToMilvusConverterV2:
    def __init__(
        self,
        db_file: str = "./milvus_lite.db",
        openai_api_key: Optional[str] = None,
        text_embedding_dim: int = 768,
    ):
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Milvus Liteë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤ (ê³µì‹ API ì‚¬ìš©)
        
        Args:
            db_file: Milvus Lite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
            openai_api_key: GPTë¥¼ ì‚¬ìš©í•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œìš© API í‚¤
            text_embedding_dim: í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›
        """
        self.db_file = db_file
        self.text_embedding_dim = text_embedding_dim
        
        # Milvus Client ì´ˆê¸°í™” (High-level API!)
        self.client = MilvusClient(db_file)
        logger.info(f"âœ… Connected to Milvus Lite at {db_file}")
        
        # HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        logger.info("Loading HuggingFace embeddings model for text...")
        self.text_embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cuda' if self._check_cuda() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.openai_client = None
        if openai_api_key:
            self.openai_client = openai.OpenAI(api_key=openai_api_key)
            logger.info("OpenAI client initialized for metadata extraction")
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_documents': 0,
            'processed_documents': 0,
            'created_collections': [],
            'processing_time': 0
        }

    def _check_cuda(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False

    def create_collection(self, collection_name: str):
        """
        ì»¬ë ‰ì…˜ ìƒì„± (MilvusClient API ì‚¬ìš©)
        """
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
        if self.client.has_collection(collection_name):
            logger.info(f"Dropping existing collection '{collection_name}'...")
            self.client.drop_collection(collection_name)
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ê°„ë‹¨í•œ API!)
        self.client.create_collection(
            collection_name=collection_name,
            dimension=self.text_embedding_dim,
            metric_type="COSINE",
            # auto_id=True,  # Primary key ìë™ ìƒì„±
        )
        
        logger.info(f"âœ… Created collection '{collection_name}' with dimension={self.text_embedding_dim}")
        self.stats['created_collections'].append(collection_name)

    def load_news_data(self, file_path: str) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ JSON íŒŒì¼ì„ ë¡œë“œí•˜ê³  í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        logger.info(f"Loading news data from {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë° ë³€í™˜
        if isinstance(raw_data, list) and len(raw_data) > 0:
            first_item = raw_data[0]
            
            # Case 1: search_result êµ¬ì¡°
            if "search_result" in first_item:
                logger.info("Detected search_result structure")
                news_data = []
                for item in raw_data:
                    if "search_result" in item:
                        news_data.extend(item["search_result"])
                return news_data
            
            # Case 2: item.documentList êµ¬ì¡°
            elif "item" in first_item and "documentList" in first_item["item"]:
                logger.info("Detected item.documentList structure")
                news_data = []
                for item in raw_data:
                    if "item" in item and "documentList" in item["item"]:
                        document_list = item["item"]["documentList"]
                        for doc in document_list:
                            standardized_doc = {
                                "date": doc.get("date", ""),
                                "title": doc.get("title", ""),
                                "text": doc.get("content", ""),
                                "doc_id": doc.get("docID", ""),
                                "url": doc.get("url", ""),
                                "source": doc.get("writerName", ""),
                                "vks": doc.get("vks", [])
                            }
                            news_data.append(standardized_doc)
                return news_data
            
            # Case 3: ì´ë¯¸ í‘œì¤€í™”ëœ êµ¬ì¡°
            else:
                logger.info("Detected standardized structure")
                return raw_data
        
        logger.warning(f"Unknown data structure in {file_path}")
        return []

    def extract_metadata_from_filename(self, filename: str) -> Tuple[str, str]:
        """íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ í† í”½ ì¶”ì¶œ"""
        filename_lower = filename.lower()
        
        if "disaster" in filename_lower:
            category = "disaster"
        elif "crime" in filename_lower:
            category = "crime"
        else:
            category = "other"
        
        topic = "unknown"
        topic_mapping = {
            "fire": "fire", "crime": "crime", "snow": "heavy snow",
            "earthquake": "earthquake", "infection": "infection",
            "traffic": "traffic accident", "rain": "heavy rain",
            "heatwave": "heatwave", "landslide": "landslide",
            "storm": "storm", "pm10": "pm10",
            "water": "water accident", "density": "density"
        }
        
        for key, value in topic_mapping.items():
            if key in filename_lower:
                topic = value
                break
        
        return category, topic

    def prepare_milvus_data(
        self,
        news_data: List[Dict],
        filename: str,
        use_gpt_metadata: bool = True,
        batch_size: int = 100
    ) -> List[Dict[str, Any]]:
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Milvus ì‚½ì… í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Returns:
            List of dicts: [{"id": ..., "vector": [...], "title": ..., ...}, ...]
        """
        file_category, file_topic = self.extract_metadata_from_filename(filename)
        
        logger.info(f"Preparing Milvus data from {len(news_data)} news items...")
        
        texts_to_embed = []
        valid_news = []
        
        # ìœ íš¨í•œ ë‰´ìŠ¤ í•„í„°ë§
        for idx, news in enumerate(news_data):
            title = news.get("title", "")
            content = news.get("text", "")
            
            if len(content.strip()) < 50:
                continue
            
            valid_news.append((idx, news, title, content))
            texts_to_embed.append(content)
        
        if not valid_news:
            logger.warning("No valid news items to process")
            return []
        
        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        logger.info(f"Generating embeddings for {len(texts_to_embed)} documents...")
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts_to_embed), batch_size), desc="Embedding batches"):
            if 0: #i>20000:
                break
            batch_texts = texts_to_embed[i:i+batch_size]
            batch_embeddings = self.text_embeddings.embed_documents(batch_texts)
            all_embeddings.extend(batch_embeddings)
        
        # Milvus ë°ì´í„° êµ¬ì„± (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹!)
        logger.info("Preparing data for Milvus insertion...")
        current_timestamp = int(time.time())
        
        milvus_data = []
        for embedding, (idx, news, title, content) in zip(all_embeddings, valid_news):
            if use_gpt_metadata and self.openai_client:
                # GPT ë©”íƒ€ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— êµ¬í˜„
                category, topic = file_category, file_topic
            else:
                category, topic = file_category, file_topic
            
            date = news.get("date", "")
            doc_id = news.get("doc_id", f"{filename}_{idx}")
            url = news.get("url", "")
            source = news.get("source", "")
            
            # Milvus ë°ì´í„° í•­ëª© (ë”•ì…”ë„ˆë¦¬!)
            data_item = {
                "id": idx,  # Primary key (integer)
                "vector": embedding,  # ë²¡í„°ëŠ” ê·¸ëŒ€ë¡œ list
                "doc_id": doc_id[:200] if doc_id else "",
                "title": title[:500] if title else "",
                "content": content[:5000] if content else "",  # ê¸¸ì´ ì œí•œ
                "date": date[:20] if date else "",
                "url": url[:500] if url else "",
                "source": source[:200] if source else "",
                "category": category[:50] if category else "",
                "topic": topic[:100] if topic else "",
                "filename": filename[:200],
                "created_at": current_timestamp
            }
            
            milvus_data.append(data_item)
        
        logger.info(f"Prepared {len(milvus_data)} items for Milvus")
        return milvus_data

    def insert_to_milvus(
        self,
        collection_name: str,
        data: List[Dict[str, Any]],
        batch_size: int = 1000
    ) -> bool:
        """
        Milvus ì»¬ë ‰ì…˜ì— ë°ì´í„° ì‚½ì… (MilvusClient API ì‚¬ìš©)
        """
        if not data:
            logger.warning("No data to insert")
            return False
        
        try:
            logger.info(f"Inserting {len(data)} items to collection '{collection_name}'...")
            
            # ë°°ì¹˜ ì‚½ì…
            for i in tqdm(range(0, len(data), batch_size), desc="Inserting batches"):
                batch_data = data[i:i+batch_size]
                
                # MilvusClient.insert()ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìŠµë‹ˆë‹¤!
                res = self.client.insert(
                    collection_name=collection_name,
                    data=batch_data
                )
                
                logger.debug(f"Inserted batch {i//batch_size + 1}: {res}")
            
            logger.info(f"âœ… Successfully inserted {len(data)} items to '{collection_name}'")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to insert data to Milvus: {e}")
            import traceback
            traceback.print_exc()
            return False

    def process_single_file(
        self,
        input_file: str,
        collection_name: str,
        use_gpt_metadata: bool = True
    ) -> bool:
        """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Milvusì— ì €ì¥"""
        start_time = time.time()
        filename = Path(input_file).name
        
        # ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        news_data = self.load_news_data(input_file)
        if not news_data:
            logger.error(f"No valid news data found in {input_file}")
            return False
        
        self.stats['total_documents'] += len(news_data)
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        self.create_collection(collection_name)
        
        # Milvus ë°ì´í„° ì¤€ë¹„
        milvus_data = self.prepare_milvus_data(news_data, filename, use_gpt_metadata)
        if not milvus_data:
            logger.error(f"No valid data prepared from {input_file}")
            return False
        
        self.stats['processed_documents'] += len(milvus_data)
        
        # ë°ì´í„° ì‚½ì…
        success = self.insert_to_milvus(collection_name, milvus_data)
        
        processing_time = time.time() - start_time
        self.stats['processing_time'] += processing_time
        
        logger.info(f"âœ… Processed {filename}: {len(milvus_data)} documents in {processing_time:.2f}s")
        
        # ì»¬ë ‰ì…˜ í†µê³„ ì¶œë ¥
        stats = self.client.get_collection_stats(collection_name)
        logger.info(f"Collection '{collection_name}' stats: {stats}")
        
        return success

    def print_summary(self):
        """ë³€í™˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ‰ Milvus Lite Conversion Summary (V2)")
        print("="*60)
        print(f"ğŸ“Š Total Documents: {self.stats['total_documents']}")
        print(f"âœ… Processed Documents: {self.stats['processed_documents']}")
        print(f"ğŸ—‚ï¸ Created Collections: {len(self.stats['created_collections'])}")
        print(f"â±ï¸ Processing Time: {self.stats['processing_time']:.2f} seconds")
        print(f"ğŸ’¾ Database File: {self.db_file}")
        print("="*60)


def main():
    parser = argparse.ArgumentParser(description='Convert news data to Milvus Lite V2 (Official API)')
    parser.add_argument('--input', '-i', required=True,
                       help='Input file path')
    parser.add_argument('--collection', '-c', required=True,
                       help='Milvus collection name')
    parser.add_argument('--db-file', default='./milvus_lite_v2.db',
                       help='Milvus Lite database file path')
    parser.add_argument('--openai-key',
                       help='OpenAI API key for metadata extraction')
    parser.add_argument('--no-gpt', action='store_true',
                       help='Disable GPT-based metadata extraction')
    
    args = parser.parse_args()
    
    openai_key = args.openai_key or os.getenv('OPENAI_API_KEY')
    
    if not openai_key and not args.no_gpt:
        logger.warning("No OpenAI API key provided. Using filename-based metadata extraction only.")
    
    converter = NewsToMilvusConverterV2(
        db_file=args.db_file,
        openai_api_key=openai_key
    )
    
    input_path = Path(args.input)
    use_gpt = not args.no_gpt and openai_key is not None
    
    if input_path.is_file():
        success = converter.process_single_file(str(input_path), args.collection, use_gpt)
    else:
        logger.error("Invalid input path")
        sys.exit(1)
    
    if success:
        converter.print_summary()
    else:
        logger.error("Conversion failed")
        sys.exit(1)


if __name__ == "__main__":
    main()

