import os
os.environ["HF_HOME"] = "/mnt/nvme02/home/tdrag/.cache/huggingface"
os.environ["TRANSFORMERS_CACHE"] = "/mnt/nvme02/home/tdrag/.cache/huggingface"

from transformers.utils import move_cache
move_cache()

from langchain_community.embeddings import HuggingFaceEmbeddings
import gradio as gr
import logging, os, re
from pathlib import Path
import datetime
import warnings
import json
import torch

from utils import analyze_qa_type

warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Transformers ê´€ë ¨ ê²½ê³  ì–µì œ
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
import transformers
transformers.logging.set_verbosity_error()

# HuggingFace ê´€ë ¨ ê²½ê³  ì–µì œ
import logging
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)
logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)

# ì¶”ê°€ ê²½ê³  ì–µì œ ì„¤ì •
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# CUDA ìµœì í™” ì„¤ì •
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "8"

# GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB ì‚¬ìš© ê°€ëŠ¥")

BASE_RETRIEVER_MODEL = "Facebook/rag-sequence-nq" # basic retriever model

logging.basicConfig(
    filename=f'vectordb_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
    )
global_manager = None
global_retriever = None
global_generator_model = "OpenAI MCQ"  # ê¸°ë³¸ ìƒì„±ê¸° ëª¨ë¸
global_retriever_model = "Facebook/rag-sequence-nq"  # ê¸°ë³¸ ê²€ìƒ‰ì ëª¨ë¸
global_tokenizer = None
global_api_key = None  # OpenAI API í‚¤ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
global_hybrid_search = None  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„

import pickle
from pydantic import BaseModel
from typing import Dict, Any
from tqdm import tqdm

# load data from other file
from utils import load_news_data
from manager import VectorStoreManager
from search import SearchInterface

from retrieval.dpr import run_dpr_question, load_model
from retrieval.gcs import search as gcs_search, parse_article

# langchain imports
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

import openai
import time

# utils.py
from utils import (
    load_news_data, create_documents, create_faiss_index, create_chunks, process_date_string, retrieve_single_question, compute_relative_date
)


# hybrid search
from hybrid_search import HybridSearchEngine, create_comprehensive_answer
from uuid import uuid4

from evaluate import accuracy, gen_eval

# keys.py
from keys import GCS_KEY, ENGINE_KEY, OPENAI_API_KEY, MODEL_PATH, MODEL_NAMES, EXTRACTOR_MODEL_PATH, COHERE_API_KEY

global_api_key = OPENAI_API_KEY  # OpenAI API í‚¤ ì„¤ì •

global_extractor_model_path = EXTRACTOR_MODEL_PATH


log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
logging.basicConfig(
    filename=log_dir / f'vectordb_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

def initialize_manager():
    """ì „ì—­ manager ë° ëª¨ë¸ ì´ˆê¸°í™”"""
    global global_manager, global_retriever, global_retriever_model, global_tokenizer, global_hybrid_search

    # GPU ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    model_name = "sentence-transformers/all-mpnet-base-v2"
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': device},  # GPU ì‚¬ìš© ì„¤ì •
        encode_kwargs={'normalize_embeddings': False}
    )

    # ê²€ìƒ‰ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ

    if global_retriever is None or global_retriever_model is None or global_tokenizer is None:
        print("Loading retrieval models...")
        retriever, retriever_model, tokenizer = load_model(
            BASE_RETRIEVER_MODEL,
            top_k=25,  # ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¬ë¡œë“œ ë°©ì§€
            device=device
        )
        global_retriever = retriever
        global_retriever_model = retriever_model
        global_tokenizer = tokenizer
        print("Retrieval models loaded successfully")


    base_dir = Path("/mnt/nvme02/home/tdrag/vaiv/RTRAG/faiss_indexes_metadata")  # FAISS DB ê²½ë¡œ - ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¸ë±ì‹±
    
    # ëª¨ë“  FAISS ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ì°¾ê¸° (ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, í† í”½ í¬í•¨)
    sub_dirs = []
    if base_dir.exists():
        for d in base_dir.iterdir():
            if d.is_dir():
                # ë‚ ì§œë³„ ì¸ë±ìŠ¤ (YYYYMM ë˜ëŠ” YYYYMMDD í˜•ì‹)
                if re.match(r"^(date_)?\d{6,8}$", d.name):
                    sub_dirs.append(d)
                # ë³‘í•©ëœ ë‚ ì§œ ì¸ë±ìŠ¤
                elif re.match(r"merged_\d{4,8}", d.name):
                    sub_dirs.append(d)
                # ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ìŠ¤
                elif d.name.startswith("category_"):
                    sub_dirs.append(d)
                # í† í”½ë³„ ì¸ë±ìŠ¤
                elif d.name.startswith("topic_"):
                    sub_dirs.append(d)
                # í†µí•© ì¸ë±ìŠ¤ (ìƒˆë¡œìš´ íš¨ìœ¨ì  ë°©ì‹)
                elif d.name.startswith("unified_"):
                    sub_dirs.append(d)

    if not sub_dirs:
        print(f"No FAISS index directories found in {base_dir}")
        print("Available directories:", [d.name for d in base_dir.iterdir() if d.is_dir()] if base_dir.exists() else "Base directory doesn't exist")
    else:
        print(f"Found {len(sub_dirs)} FAISS index directories:")
        for d in sub_dirs:
            index_type = "date" if re.match(r"^(date_)?\d{6,8}$", d.name) else \
                        "merged" if d.name.startswith("merged_") else \
                        "category" if d.name.startswith("category_") else \
                        "topic" if d.name.startswith("topic_") else \
                        "unified" if d.name.startswith("unified_") else "unknown"
            print(f"  - {d.name} ({index_type})")

    global_manager = VectorStoreManager(embeddings, base_dir)
    
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    if global_hybrid_search is None:
        # Google API í‚¤ëŠ” keys.pyì—ì„œ ê¸°ë³¸ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        try:
            from keys import GOOGLE_API_KEY, GOOGLE_CSE_ID
            google_api_key = GOOGLE_API_KEY
            google_cse_id = GOOGLE_CSE_ID
            print("âœ… Google API keys loaded from keys.py")
        except ImportError:
            # keys.pyì— ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            google_api_key = os.getenv('GOOGLE_API_KEY')
            google_cse_id = os.getenv('GOOGLE_CSE_ID')
            print("âš ï¸ Google API keys loaded from environment variables")
        
        global_hybrid_search = HybridSearchEngine(
            vector_manager=global_manager,
            google_api_key=google_api_key,
            google_cse_id=google_cse_id
        )

    # ëª¨ë¸ ì›Œë°ì—… ì‹¤í–‰
    warmup_models()

    return global_manager

# GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì—ì„œ ë©”íƒ€ë°ì´í„°(ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, í† í”½) ì¶”ì¶œ
def return_date_info(query_input):
    import time
    from typing import List, Dict, Optional, Tuple
    start_time = time.time()

    # ê²°ê³¼ í•´ì„
    results_output = "Results from query:\n"
    results_output += f"Query: {query_input}\n\n"

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    extracted_dates = []
    metadata = {
        "category": None,
        "topic": None,
        "date_range": None
    }

    try:
        import openai
        client = openai.OpenAI(api_key=global_api_key)

        # GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """You are a metadata extraction specialist. Extract metadata from the given query in JSON format with these fields:

                1. dates: Array of dates in YYYYMMDD format. If a date range is mentioned, include both start and end dates. If only one date is mentioned, include it twice. If no specific date is mentioned, return empty array.
                2. category: Main news category from this list: ['politics', 'economics', 'society', 'culture', 'technology', 'sports', 'entertainment', 'disaster', 'crime', 'health', 'environment', 'international', 'other']. Return null if unclear.
                3. topic: Specific topic or keyword (max 2 words) that best describes the main subject. Return null if unclear.

                Rules:
                - Always respond with valid JSON only
                - Use YYYYMMDD format for dates (e.g., 20240115)
                - Choose the most appropriate category from the provided list
                - Keep topics concise and specific

                Example response:
                {"dates": ["20220115", "20220201"], "category": "disaster", "topic": "heavy snow"}"""},
                {"role": "user", "content": "What are the major fire incidents in the past 3 years?"},
                {"role": "assistant", "content": '''{"dates": ["20210101", "20231231"], "category": "disaster", "topic": "fire"}'''},
                {"role": "user", "content": "COVID-19 infection trends in 2021-2022"},
                {"role": "assistant", "content": '''{"dates": ["20210101", "20221231"], "category": "disaster", "topic": "infection"}'''},
                {"role": "user", "content": "Recent earthquake news and damage reports"},
                {"role": "assistant", "content": '''{"dates": [], "category": "disaster", "topic": "earthquake"}'''},
                {"role": "user", "content": query_input}
            ],
            max_tokens=150,
            temperature=0.1
        )

        llm_results = response.choices[0].message.content.strip()
        try:
            parsed_metadata = json.loads(llm_results)

            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬
            if parsed_metadata.get("dates"):
                extracted_dates.extend(parsed_metadata["dates"])
                results_output += f"Date Range Found: {parsed_metadata['dates'][0]} to {parsed_metadata['dates'][-1]}\n"

            # ì¹´í…Œê³ ë¦¬ì™€ í† í”½ ì •ë³´ ì²˜ë¦¬
            metadata["category"] = parsed_metadata.get("category")
            metadata["topic"] = parsed_metadata.get("topic")

            if metadata["category"] or metadata["topic"]:
                results_output += "\n### Category/Topic Information\n"
                if metadata["category"]:
                    results_output += f"Category: {metadata['category']}\n"
                if metadata["topic"]:
                    results_output += f"Topic: {metadata['topic']}\n"

        except json.JSONDecodeError:
            results_output += "Error parsing metadata from GPT response.\n"

    except Exception as e:
        results_output += f"\n### Error in Date Extraction\n\nError: {str(e)}\n"

    processing_time = time.time() - start_time

    # ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì • (ë‰´ìŠ¤ ë°ì´í„° ë²”ìœ„ì— ë§ì¶¤)
    if not extracted_dates:
        end_date = "20231231"  # ë‰´ìŠ¤ ë°ì´í„° ìµœëŒ€ ë²”ìœ„
        start_date = "20210101"  # ë‰´ìŠ¤ ë°ì´í„° ì‹œì‘ ë²”ìœ„
        extracted_dates = [start_date, end_date]

    # ë‚ ì§œ ì •ë³´ ì •ë ¬ ë° ì‹œì‘/ì¢…ë£Œ ë‚ ì§œ ì„¤ì •
    extracted_dates.sort()
    date_range = f"{extracted_dates[0]}/{extracted_dates[-1]}"

    results_output += f"\n### Date Range for Search\n{date_range}\n"

    # ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜
    return results_output, date_range, metadata

# FAISS ë³€í™˜ ì „ìš© í•¨ìˆ˜
def convert_to_faiss_indexes(files, use_metadata=True, category_filter=None, topic_filter=None):
    """íŒŒì¼ë“¤ì„ FAISS ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ì „ìš© í•¨ìˆ˜"""
    global global_manager, global_api_key
    if global_manager is None:
        global_manager = initialize_manager()

    if not files:
        return None, "Please upload at least one file."

    processed_data = {
        "Number_of_indexes": 0,
        "Index_by_date": {},
        "Index_by_category": {},
        "Index_by_topic": {},
        "Current_indices": {}
    }

    start_time = time.time()
    total_docs = 0
    progress_html = ""

    try:
        for file_idx, file in enumerate(files, 1):
            progress_html += f"<p>Converting File {file_idx}/{len(files)}: {file.name} to FAISS Index</p>"
            yield processed_data, progress_html

            news_data = load_news_data(file.name)
            
            # ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬
            if "search_result" in list(news_data[0].keys()):
                news_date_temp = [news["search_result"] for news in news_data]
                news_data = [item for sublist in news_date_temp for item in sublist]
            elif "item" in list(news_data[0].keys()):
                news_data_before = news_data[0]["item"]["documentList"]
                new_news_data = []
                for d in news_data_before:
                    news_object = {
                        "date": d["date"],
                        "title": d["title"],
                        "text": d["content"],
                        "doc_id": d['docID'],
                        "url": d['url'],
                        "source": d['writerName'],
                        "vks": d['vks']
                    }
                    new_news_data.append(news_object)
                news_data = new_news_data.copy()

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ë¶„ë¥˜
            categorized_docs = {}
            topic_docs = {}
            date_docs = {}

            progress_html += f"<p>Extracting metadata and categorizing documents...</p>"
            yield processed_data, progress_html

            for news_idx, news in enumerate(tqdm(news_data, desc="Processing and categorizing news data")):
                # ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
                content = news.get('text', news.get('content', ''))
                title = news.get('title', 'No Title')
                news_date = news.get('date', '20000101')
                if isinstance(news_date, str):
                    news_date = process_date_string(news_date)
                
                # ì¹´í…Œê³ ë¦¬ ë° í† í”½ ì¶”ì¶œ (GPT ì‚¬ìš©)
                if use_metadata and global_api_key:
                    category, topic = extract_category_topic(content, title, global_api_key)
                else:
                    category, topic = "general", "general"

                # í•„í„°ë§ ì ìš©
                if category_filter and category.lower() != category_filter.lower():
                    continue
                if topic_filter and topic.lower() != topic_filter.lower():
                    continue

                # ë¬¸ì„œ ìƒì„±
                doc = Document(
                    page_content=f"Title: {title}\nContent: {content}",
                    metadata={
                        'title': title,
                        'doc_id': news.get('doc_id', str(uuid4())),
                        'date': news_date,
                        'source': news.get('source', 'Unknown'),
                        'category': category,
                        'topic': topic,
                        'url': news.get('url', '')
                    }
                )

                # ë‚ ì§œë³„ ë¶„ë¥˜
                if news_date not in date_docs:
                    date_docs[news_date] = []
                date_docs[news_date].append(doc)

                # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                if category not in categorized_docs:
                    categorized_docs[category] = []
                categorized_docs[category].append(doc)

                # í† í”½ë³„ ë¶„ë¥˜
                if topic not in topic_docs:
                    topic_docs[topic] = []
                topic_docs[topic].append(doc)

                total_docs += 1

            # ğŸš€ ìƒˆë¡œìš´ íš¨ìœ¨ì  FAISS ì¸ë±ìŠ¤ ìƒì„± ë°©ì‹
            all_docs = list(categorized_docs.values())[0] if categorized_docs else []
            if not all_docs:
                # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
                all_docs = []
                for docs in date_docs.values():
                    all_docs.extend(docs)
            
            if all_docs:
                progress_html += f"<p>ğŸš€ Creating unified FAISS index (efficient method)...</p>"
                yield processed_data, progress_html
                
                # í†µí•© ì¸ë±ìŠ¤ ìƒì„± (ëª¨ë“  ë¬¸ì„œ í¬í•¨, ë©”íƒ€ë°ì´í„°ë¡œ í•„í„°ë§)
                file_basename = file.name.split('.')[0] if hasattr(file, 'name') else 'manual_upload'
                unified_index_name = f"unified_{file_basename}"
                
                global_manager.create_index(unified_index_name, all_docs)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                processed_data["Index_by_unified"] = {unified_index_name: len(all_docs)}
                processed_data["Index_by_date"] = {date: len(docs) for date, docs in date_docs.items()}
                processed_data["Index_by_category"] = {cat: len(docs) for cat, docs in categorized_docs.items()}
                processed_data["Index_by_topic"] = {topic: len(docs) for topic, docs in topic_docs.items()}
                
                print(f"âœ… Unified index created: {unified_index_name} with {len(all_docs)} documents")
                print(f"ğŸ“Š Contains {len(date_docs)} date groups, {len(categorized_docs)} categories, {len(topic_docs)} topics")
                
                # ì„ íƒì ìœ¼ë¡œ í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ë‚ ì§œë³„ ë¶„í• ë„ ìƒì„±
                if len(all_docs) > 1000:
                    progress_html += f"<p>Creating additional date-based indexes for large dataset...</p>"
                    yield processed_data, progress_html
                    
                    for date, docs in date_docs.items():
                        if len(docs) > 50:  # ìµœì†Œ ë¬¸ì„œ ìˆ˜ ì¡°ê±´
                            index_name = f"date_{date}"
                            global_manager.create_index(index_name, docs)
                            print(f"âœ… Additional date index created: {index_name} with {len(docs)} documents")
            else:
                progress_html += f"<p style='color: red;'>âŒ No documents to create indexes</p>"
                yield processed_data, progress_html

        # í†µê³„ ê³„ì‚° (í†µí•© ì¸ë±ìŠ¤ í¬í•¨)
        unified_count = len(processed_data.get("Index_by_unified", {}))
        date_count = len(processed_data.get("Index_by_date", {}))
        category_count = len(processed_data.get("Index_by_category", {}))
        topic_count = len(processed_data.get("Index_by_topic", {}))
        
        processed_data["Number_of_indexes"] = unified_count + date_count + category_count + topic_count
        
        time_spend = time.time() - start_time
        status_msg = f"âœ… FAISS Conversion Completed (Efficient Method):\n{len(files)} files, {total_docs} documents processed.\n"
        if unified_count > 0:
            status_msg += f"ğŸš€ Created {unified_count} unified index(es) (efficient storage)\n"
            status_msg += f"ğŸ“Š Metadata coverage: {category_count} categories, {topic_count} topics across {date_count} date groups\n"
        else:
            status_msg += f"Created {date_count} date indexes, {category_count} category indexes, {topic_count} topic indexes.\n"
        status_msg += f"ğŸ“ Saved to: {global_manager.base_dir}\n"
        status_msg += f"Time spent: {time_spend:.2f} seconds"

        # ìƒì„±ëœ ì¸ë±ìŠ¤ í´ë” í™•ì¸
        created_folders = []
        if global_manager.base_dir.exists():
            for folder in global_manager.base_dir.iterdir():
                if folder.is_dir():
                    created_folders.append(folder.name)
        
        if created_folders:
            status_msg += f"\nğŸ“‚ Created folders: {', '.join(created_folders)}"
        else:
            status_msg += f"\nâš ï¸ No folders found in {global_manager.base_dir}"

        progress_html += f"<p style='color: green;'>{status_msg}</p>"
        yield processed_data, progress_html

    except Exception as e:
        error_msg = f"âŒ FAISS conversion error: {str(e)}"
        progress_html += f"<p style='color: red;'>{error_msg}</p>"
        yield None, progress_html

def get_news_data_files():
    """news_data í´ë”ì˜ JSON íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
    news_data_dir = Path("/mnt/nvme02/home/tdrag/vaiv/RTRAG/news_data")
    if not news_data_dir.exists():
        return []
    
    json_files = []
    for file_path in news_data_dir.glob("*.json"):
        file_size = file_path.stat().st_size / (1024 * 1024)  # MB
        file_info = f"{file_path.name} ({file_size:.0f}MB)"
        json_files.append((str(file_path), file_info))
    
    return sorted(json_files, key=lambda x: x[1])  # ì´ë¦„ìˆœ ì •ë ¬

def auto_convert_news_data_to_faiss(selected_files=None, use_metadata=True, 
                                   category_filter=None, topic_filter=None):
    """news_data í´ë”ì˜ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ FAISSë¡œ ë³€í™˜"""
    global global_manager, global_api_key
    
    if global_manager is None:
        global_manager = initialize_manager()
    
    news_data_dir = Path("/mnt/nvme02/home/tdrag/vaiv/RTRAG/news_data")
    
    # íŒŒì¼ ì„ íƒ ë¡œì§
    if selected_files:
        # ì„ íƒëœ íŒŒì¼ë“¤ë§Œ ì²˜ë¦¬
        files_to_process = [Path(f) for f in selected_files if Path(f).exists()]
    else:
        # ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬
        files_to_process = list(news_data_dir.glob("*.json"))
    
    if not files_to_process:
        yield None, "No files found to process."
        return
    
    processed_data = {
        "Number_of_indexes": 0,
        "Index_by_date": {},
        "Index_by_category": {},
        "Index_by_topic": {},
        "Current_indices": {},
        "Processed_files": []
    }
    
    start_time = time.time()
    total_docs = 0
    progress_html = f"<h3>ğŸš€ Auto-converting {len(files_to_process)} files from news_data/</h3>"
    
    try:
        for file_idx, file_path in enumerate(files_to_process, 1):
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            progress_html += f"<p>ğŸ“„ Processing File {file_idx}/{len(files_to_process)}: {file_path.name} ({file_size_mb:.0f}MB)</p>"
            yield processed_data, progress_html
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€
            filename = file_path.name.lower()
            auto_category = "disaster"  # ê¸°ë³¸ê°’
            auto_topic = None
            
            # íŒŒì¼ëª…ì—ì„œ í† í”½ ì¶”ì¶œ
            if "fire" in filename:
                auto_topic = "fire"
            elif "crime" in filename:
                auto_category = "crime"
                auto_topic = "crime"
            elif "snow" in filename:
                auto_topic = "heavy snow"
            elif "earthquake" in filename:
                auto_topic = "earthquake"
            elif "infection" in filename:
                auto_topic = "infection"
            elif "traffic" in filename:
                auto_topic = "traffic accident"
            elif "rain" in filename:
                auto_topic = "heavy rain"
            elif "heatwave" in filename:
                auto_topic = "heatwave"
            elif "landslide" in filename:
                auto_topic = "landslide"
            elif "storm" in filename:
                auto_topic = "storm"
            elif "pm10" in filename:
                auto_topic = "pm10"
            elif "water" in filename:
                auto_topic = "water accident"
            
            progress_html += f"<p>ğŸ·ï¸ Auto-detected: Category={auto_category}, Topic={auto_topic}</p>"
            yield processed_data, progress_html
            
            # íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
            try:
                news_data = load_news_data(str(file_path))
                
                # ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
                if news_data and isinstance(news_data, list) and len(news_data) > 0:
                    if "search_result" in list(news_data[0].keys()):
                        news_date_temp = [news["search_result"] for news in news_data]
                        news_data = [item for sublist in news_date_temp for item in sublist]
                    elif "item" in list(news_data[0].keys()):
                        news_data_before = news_data[0]["item"]["documentList"]
                        new_news_data = []
                        for d in news_data_before:
                            news_object = {
                                "date": d["date"],
                                "title": d["title"],
                                "text": d["content"],
                                "doc_id": d['docID'],
                                "url": d['url'],
                                "source": d['writerName'],
                                "vks": d['vks']
                            }
                            new_news_data.append(news_object)
                        news_data = new_news_data.copy()
                
                # ë¬¸ì„œ ì²˜ë¦¬ ë° í†µí•© ì¸ë±ìŠ¤ ìƒì„±
                file_result = process_single_news_file_unified(
                    news_data, file_path.name, use_metadata, 
                    category_filter or auto_category, 
                    topic_filter or auto_topic
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                if file_result.get("processed", False):
                    created_indexes = file_result.get("created_indexes", [])
                    processed_data["Number_of_indexes"] += len(created_indexes)
                    
                    # ì¸ë±ìŠ¤ ì •ë³´ ì—…ë°ì´íŠ¸ (í†µí•© ì¸ë±ìŠ¤ ì§€ì›)
                    for index_name in created_indexes:
                        if index_name.startswith("unified_"):
                            # í†µí•© ì¸ë±ìŠ¤ ì •ë³´ ì €ì¥
                            if "Index_by_unified" not in processed_data:
                                processed_data["Index_by_unified"] = {}
                            processed_data["Index_by_unified"][index_name] = file_result.get("documents", 0)
                            
                            # ë©”íƒ€ë°ì´í„° í†µê³„ë„ ì €ì¥
                            metadata_stats = file_result.get("metadata_stats", {})
                            processed_data["metadata_coverage"] = metadata_stats
                            
                        elif index_name.startswith("date_"):
                            date_key = index_name.replace("date_", "")
                            processed_data["Index_by_date"][date_key] = index_name
                        elif index_name.startswith("category_"):
                            cat_key = index_name.replace("category_", "")
                            processed_data["Index_by_category"][cat_key] = index_name
                        elif index_name.startswith("topic_"):
                            topic_key = index_name.replace("topic_", "")
                            processed_data["Index_by_topic"][topic_key] = index_name
                    
                    processed_data["Processed_files"].append({
                        "filename": file_path.name,
                        "size_mb": file_size_mb,
                        "documents": file_result.get("documents", 0),
                        "category": auto_category,
                        "topic": auto_topic,
                        "created_indexes": created_indexes,
                        "status": "success"
                    })
                    
                    total_docs += file_result.get("documents", 0)
                    progress_html += f"<p>âœ… Created {len(created_indexes)} indexes from {file_path.name} ({file_result.get('documents', 0)} documents)</p>"
                    progress_html += f"<p>ğŸ“‚ Indexes: {', '.join(created_indexes)}</p>"
                else:
                    processed_data["Processed_files"].append({
                        "filename": file_path.name,
                        "size_mb": file_size_mb,
                        "documents": 0,
                        "category": auto_category,
                        "topic": auto_topic,
                        "error": file_result.get("error", "Unknown error"),
                        "status": "failed"
                    })
                    progress_html += f"<p style='color: red;'>âŒ Failed to process {file_path.name}: {file_result.get('error', 'Unknown error')}</p>"
                
                yield processed_data, progress_html
                
            except Exception as e:
                error_msg = f"âŒ Error processing {file_path.name}: {str(e)}"
                progress_html += f"<p style='color: red;'>{error_msg}</p>"
                yield processed_data, progress_html
                continue
        
        # ìµœì¢… ê²°ê³¼
        time_spend = time.time() - start_time
        total_indexes = processed_data["Number_of_indexes"]
        
        status_msg = f"ğŸ‰ Auto-conversion completed (Efficient Method)!\n"
        status_msg += f"ğŸ“Š Processed {len(files_to_process)} files, {total_docs} total documents\n"
        status_msg += f"ğŸ—‚ï¸ Created {total_indexes} FAISS indexes\n"
        
        # í†µí•© ì¸ë±ìŠ¤ ì •ë³´ í‘œì‹œ
        unified_indexes = processed_data.get("Index_by_unified", {})
        if unified_indexes:
            status_msg += f"ğŸš€ Unified indexes: {len(unified_indexes)} (efficient storage)\n"
            for index_name, doc_count in unified_indexes.items():
                status_msg += f"   â€¢ {index_name}: {doc_count} documents\n"
            
            # ë©”íƒ€ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í‘œì‹œ
            metadata_coverage = processed_data.get("metadata_coverage", {})
            if metadata_coverage:
                status_msg += f"ğŸ“Š Metadata coverage: {metadata_coverage.get('categories', 0)} categories, "
                status_msg += f"{metadata_coverage.get('topics', 0)} topics, {metadata_coverage.get('date_groups', 0)} date groups\n"
        else:
            # ê¸°ì¡´ ë°©ì‹ ì •ë³´
            status_msg += f"ğŸ“… Date indexes: {len(processed_data['Index_by_date'])}\n"
            status_msg += f"ğŸ·ï¸ Category indexes: {len(processed_data['Index_by_category'])}\n"
            status_msg += f"ğŸ”– Topic indexes: {len(processed_data['Index_by_topic'])}\n"
        status_msg += f"â±ï¸ Time spent: {time_spend:.2f} seconds\n"
        status_msg += f"ğŸ“ Indexes saved to: {global_manager.base_dir}"
        
        # í˜„ì¬ ì¸ë±ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸
        if global_manager:
            processed_data["Current_indices"] = global_manager.load_created_indexes()
        
        progress_html += f"<p style='color: green; font-weight: bold;'>{status_msg}</p>"
        
        # ìƒì„±ëœ ì¸ë±ìŠ¤ ëª©ë¡ í‘œì‹œ
        if total_indexes > 0:
            progress_html += "<h4>ğŸ“‚ Created Indexes:</h4><ul>"
            for date_key, index_name in processed_data["Index_by_date"].items():
                progress_html += f"<li>ğŸ“… {index_name}</li>"
            for cat_key, index_name in processed_data["Index_by_category"].items():
                progress_html += f"<li>ğŸ·ï¸ {index_name}</li>"
            for topic_key, index_name in processed_data["Index_by_topic"].items():
                progress_html += f"<li>ğŸ”– {index_name}</li>"
            progress_html += "</ul>"
        
        yield processed_data, progress_html
        
    except Exception as e:
        error_msg = f"âŒ Auto-conversion failed: {str(e)}"
        progress_html += f"<p style='color: red;'>{error_msg}</p>"
        yield None, progress_html

def process_single_news_file_unified(news_data, filename, use_metadata, category_filter, topic_filter):
    """ë‹¨ì¼ ë‰´ìŠ¤ íŒŒì¼ì„ í†µí•© ì¸ë±ìŠ¤ë¡œ ì²˜ë¦¬"""
    global global_manager, global_api_key
    
    if not news_data:
        return {"processed": False, "documents": 0, "error": "No data"}
    
    try:
        # Document ê°ì²´ ìƒì„±
        documents = []
        for idx, news in enumerate(news_data):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if use_metadata and global_api_key:
                try:
                    category, topic = extract_category_topic(
                        news.get('text', ''), 
                        news.get('title', ''), 
                        global_api_key
                    )
                except:
                    category = category_filter or "unknown"
                    topic = topic_filter or "unknown"
            else:
                category = category_filter or "unknown"
                topic = topic_filter or "unknown"
            
            # Document ìƒì„±
            from langchain_core.documents import Document
            doc = Document(
                page_content=news.get('text', ''),
                metadata={
                    'title': news.get('title', ''),
                    'date': news.get('date', ''),
                    'source': news.get('source', ''),
                    'url': news.get('url', ''),
                    'category': category,
                    'topic': topic,
                    'doc_id': news.get('doc_id', f"{filename}_{idx}")
                }
            )
            documents.append(doc)
        
        if not documents:
            return {"processed": False, "documents": 0, "error": "No documents created"}
        
        # í†µí•© ì¸ë±ìŠ¤ ìƒì„± (ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ì—)
        file_basename = filename.split('.')[0]
        unified_index_name = f"unified_{file_basename}"
        
        try:
            global_manager.create_index(unified_index_name, documents)
            created_indexes = [unified_index_name]
            print(f"âœ… Created unified index: {unified_index_name} with {len(documents)} documents")
            
            # ë©”íƒ€ë°ì´í„° í†µê³„ ìˆ˜ì§‘
            categories = set(doc.metadata.get('category', 'unknown') for doc in documents)
            topics = set(doc.metadata.get('topic', 'unknown') for doc in documents if doc.metadata.get('topic') != 'unknown')
            dates = set(doc.metadata.get('date', '')[:6] for doc in documents if len(doc.metadata.get('date', '')) >= 6)
            
            print(f"ğŸ“Š Metadata coverage: {len(categories)} categories, {len(topics)} topics, {len(dates)} date groups")
            
            return {
                "processed": True, 
                "documents": len(documents),
                "created_indexes": created_indexes,
                "metadata_stats": {
                    "categories": len(categories),
                    "topics": len(topics),
                    "date_groups": len(dates)
                }
            }
        except Exception as e:
            print(f"âŒ Error creating unified index {unified_index_name}: {e}")
            return {"processed": False, "documents": len(documents), "error": str(e)}
        
    except Exception as e:
        print(f"âŒ Error in process_single_news_file_unified: {e}")
        return {"processed": False, "documents": 0, "error": str(e)}

def process_single_news_file(news_data, filename, use_metadata, category_filter, topic_filter):
    """ë‹¨ì¼ ë‰´ìŠ¤ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    global global_manager, global_api_key
    
    if not news_data:
        return {"processed": False, "documents": 0, "error": "No data"}
    
    try:
        # Document ê°ì²´ ìƒì„±
        documents = []
        for idx, news in enumerate(news_data):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if use_metadata and global_api_key:
                try:
                    category, topic = extract_category_topic(
                        news.get('text', ''), 
                        news.get('title', ''), 
                        global_api_key
                    )
                except:
                    category = category_filter or "unknown"
                    topic = topic_filter or "unknown"
            else:
                category = category_filter or "unknown"
                topic = topic_filter or "unknown"
            
            # Document ìƒì„±
            from langchain_core.documents import Document
            doc = Document(
                page_content=news.get('text', ''),
                metadata={
                    'title': news.get('title', ''),
                    'date': news.get('date', ''),
                    'source': news.get('source', ''),
                    'url': news.get('url', ''),
                    'category': category,
                    'topic': topic,
                    'doc_id': news.get('doc_id', f"{filename}_{idx}")
                }
            )
            documents.append(doc)
        
        if not documents:
            return {"processed": False, "documents": 0, "error": "No documents created"}
        
        # ë‚ ì§œë³„ ì¸ë±ìŠ¤ ìƒì„±
        date_groups = {}
        for doc in documents:
            doc_date = doc.metadata.get('date', '')
            if len(doc_date) >= 6:  # YYYYMM í˜•ì‹ìœ¼ë¡œ ê·¸ë£¹í™”
                date_key = doc_date[:6]  # YYYYMM
            else:
                date_key = "unknown"
            
            if date_key not in date_groups:
                date_groups[date_key] = []
            date_groups[date_key].append(doc)
        
        # ê° ë‚ ì§œ ê·¸ë£¹ë³„ë¡œ ì¸ë±ìŠ¤ ìƒì„±
        created_indexes = []
        for date_key, date_docs in date_groups.items():
            index_name = f"date_{date_key}"
            try:
                global_manager.create_index(index_name, date_docs)
                created_indexes.append(index_name)
                print(f"âœ… Created date index: {index_name} with {len(date_docs)} documents")
            except Exception as e:
                print(f"âŒ Error creating date index {index_name}: {e}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ìŠ¤ ìƒì„±
        category_groups = {}
        for doc in documents:
            cat = doc.metadata.get('category', 'unknown')
            if cat not in category_groups:
                category_groups[cat] = []
            category_groups[cat].append(doc)
        
        for cat, cat_docs in category_groups.items():
            index_name = f"category_{cat}"
            try:
                global_manager.create_index(index_name, cat_docs)
                created_indexes.append(index_name)
                print(f"âœ… Created category index: {index_name} with {len(cat_docs)} documents")
            except Exception as e:
                print(f"âŒ Error creating category index {index_name}: {e}")
        
        # í† í”½ë³„ ì¸ë±ìŠ¤ ìƒì„±
        topic_groups = {}
        for doc in documents:
            topic = doc.metadata.get('topic', 'unknown')
            if topic and topic != 'unknown':
                if topic not in topic_groups:
                    topic_groups[topic] = []
                topic_groups[topic].append(doc)
        
        for topic, topic_docs in topic_groups.items():
            index_name = f"topic_{topic.replace(' ', '_')}"
            try:
                global_manager.create_index(index_name, topic_docs)
                created_indexes.append(index_name)
                print(f"âœ… Created topic index: {index_name} with {len(topic_docs)} documents")
            except Exception as e:
                print(f"âŒ Error creating topic index {index_name}: {e}")
        
        return {
            "processed": True, 
            "documents": len(documents),
            "created_indexes": created_indexes,
            "date_groups": len(date_groups),
            "category_groups": len(category_groups),
            "topic_groups": len(topic_groups)
        }
        
    except Exception as e:
        print(f"âŒ Error in process_single_news_file: {e}")
        return {"processed": False, "documents": 0, "error": str(e)}

def extract_category_topic(content, title, api_key):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸ ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ í† í”½ ì¶”ì¶œ"""
    try:
        import openai
        client = openai.OpenAI(api_key=api_key)
        
        text_sample = f"Title: {title}\nContent: {content[:500]}..."  # ì²˜ìŒ 500ìë§Œ ì‚¬ìš©
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """You are a news categorization expert. Analyze the given news content and extract category and topic in JSON format.

                Instructions:
                - category: Choose ONE from ['politics', 'economics', 'society', 'culture', 'technology', 'sports', 'entertainment', 'disaster', 'crime', 'health', 'environment', 'international', 'other']
                - topic: A specific keyword or phrase (max 2 words) that best describes the main subject
                
                Rules:
                - Always respond with valid JSON only
                - Be precise and concise
                - Choose the most relevant category
                
                Example responses:
                {"category": "disaster", "topic": "fire"}
                {"category": "disaster", "topic": "earthquake"}
                {"category": "disaster", "topic": "heavy snow"}
                {"category": "disaster", "topic": "infection"}
                {"category": "disaster", "topic": "traffic accident"}"""},
                {"role": "user", "content": f"Categorize this news content:\n\n{text_sample}"}
            ],
            max_tokens=80,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content.strip())
        return result.get("category", "other"), result.get("topic", "general")
        
    except Exception as e:
        print(f"Category/Topic extraction error: {e}")
        return "other", "general"

# FAISS DBì— ë„£ê¸°
def process_uploaded_files(files, use_type='Retriever with Metadata', question_type='Generate', use_faiss=True):
    global global_manager, global_api_key
    if global_manager is None:
        global_manager = initialize_manager()

    if not files:
        return None, "Please upload at least one file."

    processed_data = {
        "Number_of_indexes": len(files),
        "Index_by_date": {},
        "Current_indices": {}
    }

    start_time = time.time()

    total_docs = 0
    progress_html = ""
    processed_list = [] # tlfgod

    # í•¨ìˆ˜ í˜¸ì¶œ
    from utils import process_openai_generate, process_openai_mcq

    try:
        for file_idx, file in enumerate(files, 1):

            search_interface = SearchInterface(global_manager)
            search_interface.openai_api_key = global_api_key

            import openai
            client = openai.OpenAI(api_key=global_api_key)

            progress_html += f"<p>Processing File {file_idx}/{len(files)}: {file.name}, Process : {use_type}</p>"
            yield processed_data, progress_html

            news_data = load_news_data(file.name)

            # news_data key analysis
            assert isinstance(news_data, list), "News data should be a list."
            assert all(isinstance(news, dict) for news in news_data), "Each news item should be a dictionary."
            print(news_data[0].keys())

            # news_dataë¥¼ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "search_result" in list(news_data[0].keys()):
                    print("NEWSKEYS")
                    assert "text" in news_data[0]["search_result"][0], "ë‰´ìŠ¤ ë°ì´í„°ì˜ 'search_result' í•­ëª©ì— 'text' í‚¤ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                    news_date_temp = [news["search_result"] for news in news_data]
                    news_data = [item for sublist in news_date_temp for item in sublist]  # flatten list
                    with open("news_data_temp_flattened.json", "w", encoding="utf-8") as f:
                        json.dump(news_data, f, ensure_ascii=False, indent=4)
            elif "item" in list(news_data[0].keys()):
                # form {success, code, message item -> {keyword, totalCnt, documentList}}
                assert "content" in news_data[0]["item"]["documentList"][0], "ë‰´ìŠ¤ ë°ì´í„°ì˜ item í•­ëª©ì— 'content' í‚¤ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
                news_data_before = news_data[0]["item"]["documentList"] # ì¼ë‹¨ ì •ë¦¬
                new_news_data = []
                # í’€ê¸° ì‰½ê²Œ ì¬ì •ë¦¬
                for d in news_data_before:
                    news_object = {
                        "date": d["date"],
                        "title": d["title"],
                        "text": d["content"],
                        "doc_id": d['docID'],
                        "url": d['url'],
                        "source": d['writerName'],
                        "vks": d['vks']
                    }
                    new_news_data.append(news_object)
                news_data = new_news_data.copy()

            # ë¶„ì„ ì‹œê³„ì—´ ë°”ê¾¸ê¸°
            analyzed_news_data = []
            for news_idx, news in enumerate(tqdm(news_data, desc="Processing news data")):
                analyzed_news = analyze_qa_type(news, qa_name="realtimeqa", question_type=question_type, use_type=use_type)
                analyzed_news_data.append(analyzed_news)

            use_metadata = True if use_type.lower() in ['retriever with metadata', 'no retriever with metadata'] else False
            use_retriever = True if use_type.lower() in ['retriever with metadata', 'retriever with no metadata', 'retriever only'] else False

            # use_typeì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ê²°ì • - Retrieveë¡œ ì‹œì‘í•  ë•Œ ì €ì¥
            # use_retrieverê°€ Trueì´ë©´ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            search_list = [] # ê° ë‰´ìŠ¤ë§ˆë‹¤ ëŒ€ì‘í•˜ê¸° analyzed news_dataì™€ 1:1 ëŒ€ì‘
            if use_retriever:
                search_interface.retriever = global_retriever
                search_interface.retriever_model = global_retriever_model
                search_interface.tokenizer = global_tokenizer

                for news_idx, news in enumerate(tqdm(analyzed_news_data, desc="Processing news data")):
                    print(f"[DEBUG] Analyzed news for index {news_idx}: {news}")
                    # query -> gcs_search ì‚¬ìš©í•´ì„œ
                    query = news.get("query", "")
                    if not query:
                        search_list.append([])
                        continue
                    # find search_result from query by retrieve_single_question
                    top_k = 5
                    # use_metadataì— ë”°ë¼ ë‚ ì§œ ì„¤ì •
                    if use_metadata:
                        end_date = news.get('date', '20000101')
                        if isinstance(end_date, str):
                            end_date = process_date_string(end_date)
                        elif isinstance(end_date, datetime.date):
                            end_date = end_date.strftime("%Y%m%d")
                        else:
                            end_date = '20000101_nm'
                        if re.match(r"20[0-2][0-9][01][0-9][0-3][0-9]", end_date):
                            start_date = compute_relative_date(end_date, -30)  # 30ì¼ ì „
                        else:
                            start_date = None
                        # queryì—ì„œë„ ë‚ ì§œ ì •ë³´ ì‚¬ìš©í•´ì„œ
                        # query_context = return_date_info(query, use_heidel_time=False, use_llm=True)
                        # query = f"Time Metadata : {query_context} is given. \nNow answer the question with given metadata  {query}" # ì§ˆë¬¸ì— time_metadata ì •ë³´ ì‚½ì…
                    else:
                        end_date = '20000101_nm'
                        start_date = None

                    # BM25 ê°•í™” ê²€ìƒ‰ ì‚¬ìš©
                    try:
                        from utils import retrieve_single_question_with_bm25
                        search_result = retrieve_single_question_with_bm25(
                            query, global_retriever_model, global_retriever, global_tokenizer, GCS_KEY, ENGINE_KEY,
                            top_k=10, start_date=start_date, end_date=end_date, use_metadata=use_metadata,
                            use_reranking=True, use_bm25=True,
                            rerank_method="cohere" if COHERE_API_KEY else "custom",
                            rerank_api_key=COHERE_API_KEY or OPENAI_API_KEY,
                            chunk_size=1000, chunk_overlap=500
                        )
                        print(f"ğŸš€ BM25-enhanced search completed for query: {query}")
                    except ImportError:
                        # BM25 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                        print("âš ï¸ BM25 not available, using standard retrieval")
                        search_result = retrieve_single_question(
                            query, global_retriever_model, global_retriever, global_tokenizer, GCS_KEY, ENGINE_KEY,
                            top_k=10, start_date=start_date, end_date=end_date, use_metadata=use_metadata,
                            use_reranking=True,
                            rerank_method="cohere" if COHERE_API_KEY else "custom",
                            rerank_api_key=COHERE_API_KEY or OPENAI_API_KEY,
                            chunk_size=1000, chunk_overlap=500
                        )

                    if not search_result:
                        print(f"No search result for query: {query}")
                        search_list.append([])
                        continue
                    else:
                        print(f"Search result for query '{query}': {search_result}")
                        search_list.append(search_result)

                # search_list -> FAISS DBì— ì €ì¥. ì¤‘ë³µì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
                if use_metadata and use_faiss:
                    # ë‚ ì§œë³„ë¡œ ì¸ë±ì‹± ë° ë¬¸ì„œ ì²˜ë¦¬
                    progress_html_new = progress_html + f"<p>Creating Indexes... ({len(search_list)})</p>"
                    yield processed_data, progress_html_new

                    # search_list -> ë¬¸ì„œ ìƒì„±
                    documents = []
                    for idx, (search_result, news) in enumerate(tqdm(zip(search_list, analyzed_news_data), desc="Creating documents from search results")):
                        if not search_result:
                            logging.warning(f"No search results for index {idx} in {file.name}. Skipping.")
                            continue
                        for result in search_result:
                            doc = Document(
                                page_content=result.get('text', 'No Text'),
                                metadata={
                                    'title': result.get('title', 'No Title'),
                                    'doc_id': result.get('doc_id', 'No Doc ID'),
                                    'query': query,
                                    'date': result.get('date', '20000101'),
                                    'source': result.get('source', 'None')
                                }
                            )
                            documents.append(doc)
                            # ë¬¸ì„œëª… í˜¸ì¶œ - faiss_indexes_metadata/{date}_{source} - example : 20250601_CNN
                            logging.info(f"Document created for query '{query}': {doc.metadata['title']} (ID: {doc.metadata['doc_id']})")
                            index_name = f"{result.get('date', '20000101')}_{result.get('source', 'None').replace(' ', '_')}"
                            
                            # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ë¬¸ì„œ ì¶”ê°€
                            if not global_manager.has_index(index_name):
                                global_manager.create_index(index_name, documents=[doc])
                                print(f"âœ… Created new index: {index_name}")
                            else:
                                # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€ (ì´ ë¶€ë¶„ì€ ë³„ë„ êµ¬í˜„ í•„ìš”)
                                logging.info(f"Index {index_name} already exists. Adding documents to it.")

                            processed_data["Current_indices"][index_name] = len(documents)
                            total_docs += len(documents)

                            progress_html_new = progress_html + f"<p>Added {len(documents)} documents to the index.</p>"
                            yield processed_data, progress_html_new
                            print(f"Total documents in index {index_name}: {len(documents)}")

                # ë©”íƒ€ë°ì´í„° ì—†ì„ë•ŒëŠ” 20000000_nm ì¸ë±ìŠ¤ì— ì¶”ê°€
                elif not use_metadata and use_faiss:
                    index_name = "20000000_nm"
                    # documents ì¬ì •ì˜
                    documents = []
                    # if not global_manager.has_index(index_name):
                    #    global_manager.create_index(index_name, documents=[], ids=[])
                    # documents ìƒì„± -
                    for idx, (search_result, news) in enumerate(tqdm(zip(search_list, analyzed_news_data), desc="Creating documents from search results")):
                        if not search_result:
                            logging.warning(f"No search results for index {idx} in {file.name}. Skipping.")
                            continue
                        for result in search_result:
                            doc = Document(
                                page_content=result.get('text', 'No Text'),
                                metadata={
                                    'title': result.get('title', 'No Title'),
                                    'doc_id': result.get('doc_id', 'No Doc ID'),
                                    'query': query,
                                    'date': '20000000_nm',
                                    'source': 'None'
                                }
                            )
                            documents.append(doc)
                            # ë¬¸ì„œëª… í˜¸ì¶œ - faiss_indexes_metadata/{date}_{source} - example : 20250601_CNN
                            logging.info(f"Document created for query '{query}': {doc.metadata['title']} (ID: {doc.metadata['doc_id']})")
                    global_manager.create_index(index_name, documents)
                    processed_data["Current_indices"][index_name] = len(documents)
                    total_docs += len(documents)
                    progress_html_new = progress_html + f"<p>Added {len(documents)} documents to the index.</p>"
                    yield processed_data, progress_html_new
                    print(f"âœ… Created index {index_name} with {len(documents)} documents")


            # ë‹µë³€ ì²˜ë¦¬ - ëª¨ë“  ê²½ìš°ì— í•´ê²°
            # elif use_type.lower() in ['qa', 'realtimeqa', 'realtime', 'cnnqa', 'newsqa']:
            answers = [] # ë‹µë³€ ëª©ë¡
            scores = []
            answer_objs = [] # ì „ì²´ ëª©ë¡
            context = "" # ìš°ì„  ê°€ì ¸ì˜¤ì§€ ì•Šì„ ë•ŒëŠ” ë¹ˆë¬¸ìì—´ë¡œ ì²˜ë¦¬


            progress_html += f"<p>Starting QA processing for {len(news_data)} questions...</p>"
            yield processed_data, progress_html


            for news_idx, news in enumerate(tqdm(analyzed_news_data, desc="Processing news data")):
                res_obj = dict() # ë‹µë³€ í˜•ì‹ í™•ì¸í•˜ê¸°
                # key - id, query, answers
                # ë‹µë³€ êµ¬í•˜ê¸°
                query = news.get('query') #
                if not query:
                    print(f"No query found for news: {news}")
                    continue

                # context ì •ì˜
                if use_retriever:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
                    search_result = search_list[news_idx]
                    if not search_result:
                        print(f"No search result for query: {query}")
                        continue
                    context = "\n".join([f"{item.get('title', 'idea')}: {item.get('text', '')}" for item in search_result])
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                    if use_metadata:
                        # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ contextë¥¼ chatgptì˜ ë©”íƒ€ë°ì´í„° ì…ë ¥ í•¨ìˆ˜ ì‚¬ìš©.
                        context = return_date_info(query, use_heidel_time=True, use_llm=True)
                    else:
                        context = ""

                res_obj['id'] = news.get('id', f"{news_idx}_{uuid4().hex[:8]}")  # id ìƒì„±
                res_obj['query'] = query
                res_obj['score'] = 0.5 # ê¸°ë³¸ ì ìˆ˜ ì„¤ì •
                # find answer from query
                if question_type.lower() == "generate":
                    # ìƒì„±í˜• ì§ˆë¬¸
                    answer = process_openai_generate(
                        query,
                        context,
                        client=client
                    )
                elif question_type.lower() == "mcq":
                    # ì„ ë‹¤í˜• ì§ˆë¬¸
                    answer = process_openai_mcq(
                        query,
                        context,
                        choices=news.get('choices', []),
                        client=client
                    )
                else:
                    print(f"Unsupported question type: {question_type}")
                    continue

                if not answer:
                    answer = ["0"]  # ê¸°ë³¸ê°’ ì„¤ì •
                elif isinstance(answer, str):
                    answer = [answer]

                res_obj['answer'] = answer
                res_obj['prediction'] = answer  # ì˜ˆì¸¡ ê²°ê³¼ë¡œ ë‹µë³€ ì‚¬ìš©
                answers.append(answer)
                answer_objs.append(res_obj)

                part_progress_html = progress_html + f"<p>Processed question {news_idx + 1}/{len(analyzed_news_data)}: {query}</p>"
                yield processed_data, part_progress_html



            # ì •í™•ë„ accuracy ì‚¬ìš©
            try:
                print(f"Evaluating {len(answer_objs)} answers against {len(news_data)} news data...")
                print(answer_objs[:5])  # ë””ë²„ê¹…ìš© ì¶œë ¥
                print(news_data[:5])  # ë””ë²„ê¹…ìš© ì¶œë ¥

                #pred length
                print("Length of answer_objs and news_data:", len(answer_objs), len(news_data))

                # global_generator_modelì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                model_name = global_generator_model
                if isinstance(global_generator_model, list):
                    model_name = global_generator_model[0] if global_generator_model else "openai mcq"
                elif not isinstance(global_generator_model, str):
                    model_name = str(global_generator_model)

                if question_type == "MCQ":
                    eval_results = accuracy(answer_objs, news_data) # ì •í™•ë„ ì„ ë‹¤í˜•
                elif question_type == "Generate":
                    eval_results = gen_eval(answer_objs, news_data) # ì£¼ê´€ì‹

                # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                if isinstance(eval_results, dict):
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì£¼ìš” ì •ë³´ë§Œ ì¶”ì¶œ
                    total_questions = eval_results.get('total', len(news_data))
                    if question_type == "MCQ":
                        accuracy_score = eval_results.get('accuracy', 'N/A')
                        correct_answers = eval(accuracy_score) * int(total_questions) if isinstance(accuracy_score, str) else accuracy_score * int(total_questions)
                        # correct_answers = eval_results.get('correct', 'N/A')
                        result_str = f"File: {file.name} - Accuracy: {accuracy_score}, Correct: {correct_answers}/{total_questions}"
                    elif question_type == "Generate":
                        em_score = eval_results.get('em', 0)
                        f1_score = eval_results.get('f1', 0)
                        result_str = f"File: {file.name} - EM: {em_score}, F1: {f1_score}, Total: {total_questions}"

                else:
                    result_str = f"File: {file.name} - Result: {str(eval_results)}"

                processed_list.append(result_str)
                print(f"âœ… Evaluation completed for {file.name}: {result_str}")

                if question_type == "MCQ":
                    accuracy_report_file = f"results/accuracy_report_{file.name.split('/')[-1]}"
                elif question_type == "Generate":
                    accuracy_report_file = f"results/accuracy_report_{file.name.split('/')[-1].replace('.json', '_gen.json')}"
                accuracy_reports = make_accuracy_reports(answer_objs, news_data, file_name=accuracy_report_file)
                print("accuracy_reports")
                for report in accuracy_reports[:5]:
                    print(report)

            except Exception as eval_error:
                error_str = f"File: {file.name} - Evaluation Error: {str(eval_error)}"
                processed_list.append(error_str)
                print(f"âš ï¸ Evaluation failed for {file.name}: {eval_error}")


        status_msg = f"? Processed Finished:\n {len(files)} files, {total_docs} documents processed."

        status_msg += f"\n{len(processed_list)} evaluations completed."
        time_spend = time.time() - start_time # ì†Œìš”ì‹œê°„ (ì´ˆë¡œ í‘œí˜„)
        if processed_list:
            joined_list = '\n'.join(processed_list)
            status_msg += f"\nEvaluation Results:\n{joined_list} \nTime spent: {time_spend:.2f} seconds"

        progress_html += f"<p style='color: green;'>{status_msg}</p>"

        # ëª¨ë“  ê²½ìš°ì— yieldë¡œ ë°˜í™˜ (ì¼ê´€ì„± ìœ ì§€)
        yield processed_data, progress_html


    except Exception as e:
        error_msg = f"? ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        progress_html += f"<p style='color: red;'>{error_msg}</p>"
        yield None, progress_html


def make_accuracy_reports(pred_data, gold_data, file_name="results/metadata_extraction.jsonl"):
    """pred_dataì˜ ì •ë‹µ ê²°ê³¼ì™€ gold_dataì˜ ì •ë‹µ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    assert len(pred_data) == len(gold_data), "Prediction and gold data must have the same length."
    accuracy_results = []
    obj_format = {
        "question_id": "",
        "type": "mcq", #mcq or generate
        "prediction": [],
        "answer": [],
        "em": 0, # exact match or choice match(correct choice)
        "f1": 0, # f1 score
        "score": 0.0, # score
    }
    for pred, gold in zip(pred_data, gold_data):
        from evaluate import exact_match_score, f1_score
        import itertools
        res_obj = obj_format.copy()
        res_obj["question_id"] = pred.get("question_id", gold.get("question_id", "unknown"))
        res_obj["prediction"] = pred.get("prediction", [])
        res_obj["type"] = "mcq" if str(res_obj["prediction"][0]).isnumeric() else "generate" # íƒ€ì…
        res_obj["score"] = float(pred.get("score", 0.0))  # score ê°’ ì„¤ì •
        if res_obj["type"] == "mcq":
            res_obj["answer"] = gold.get("answer", [])
            res_obj["em"] = int(res_obj["prediction"][0] in res_obj["answer"])
        else:
            answer_choices = gold.get("choices", [])
            answer_num = gold.get("answer", [])
            if not answer_choices or not answer_num:
                res_obj["answer"] = []
            else:
                pred = pred.get("prediction", [""])
                golds = [gold["choices"][int(idx)] for idx in gold["answer"]]
                golds = [' '.join(perm) for perm in list(itertools.permutations(golds))]
                res_obj["answer"] = [answer_choices[int(num)] for num in answer_num]
                res_obj["em"] = exact_match_score(pred, golds)
                res_obj["f1"] = f1_score(pred, golds)
        accuracy_results.append(res_obj)

    accuracy_objs = [json.dumps(res, ensure_ascii=False) for res in accuracy_results]

    with open(file_name, 'w', encoding='utf-8') as f:
        f.write("\n".join(accuracy_objs))

    print(f"Accuracy report saved to {file_name}")
    return accuracy_results

# ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_gradio_interface():
    global global_manager, global_api_key

    # OpenAI API í‚¤ ì´ˆê¸°í™”
    global_api_key = OPENAI_API_KEY

    global_manager = initialize_manager()
    search_interface = SearchInterface(global_manager)
    # ìµœëŒ€ ê¸¸ì´ ì§€ì •
    max_length = 4000  # ìµœëŒ€ ê¸¸ì´ ì„¤ì • (í† í° ìˆ˜ì— ë”°ë¼

    # Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    with gr.Blocks(title="Find Data from Query") as demo:
        gr.Markdown("""## Find Data from Query""")

        # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
        with gr.Row():
            system_status = gr.HTML(value=f"""
                <div style='padding: 10px; background-color: #f0f0f0; border-radius: 5px;'>
                    <b>System Status:</b><br>
                    ğŸ’» Device: {'ğŸŸ¢ GPU (CUDA)' if torch.cuda.is_available() else 'ğŸŸ¡ CPU'}<br>
                    ğŸ§  Models: {'ğŸŸ¢ Loaded' if global_retriever else 'ğŸ”´ Not Loaded'}<br>
                    ğŸ“š Embeddings: {'ğŸŸ¢ Ready' if global_manager else 'ğŸ”´ Not Ready'}
                </div>
            """)

        with gr.Group(visible=True) as api_settings:
            with gr.Row():
                api_key = gr.Textbox(label="OpenAI API Key", type="password")
                google_api_key = gr.Textbox(label="Google API Key (Optional)", type="password", placeholder="For hybrid search")
            with gr.Row():
                google_cse_id = gr.Textbox(label="Google CSE ID (Optional)", placeholder="For hybrid search")

        model_status = gr.Textbox(label="Model status", interactive=False)
        init_model_btn = gr.Button("Initialize Model")

        # ì‹¤í—˜ ì„¤ì • ì„¹ì…˜ (QA í‰ê°€ìš©)
        gr.Markdown("### Experiment Configuration")
        with gr.Row():
            with gr.Column():
                exp_type = gr.Radio(
                    choices=["No Retriever and No Metadata", "No Retriever with Metadata", "Retriever Only", "Retriever with Metadata"],
                    label="Select the type of experiment",
                    value="Retriever with Metadata"
                )
                question_type = gr.Radio(
                    choices=["MCQ", "Generate"],
                    label="Select the type of question",
                    value="Generate"
                )
                # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
                date_range = gr.Textbox(label="Date Range - YYMMDD/YYMMDD")

            with gr.Column():
                file_output = gr.JSON(label="Current File Status")
                upload_button = gr.File(
                    label="Upload JSON/JSONL files for QA Evaluation",
                    file_types=[".json", ".jsonl"],
                    file_count="multiple",
                )
                status_output = gr.HTML(label="Status Output")

        # FAISS ë³€í™˜ ì„¹ì…˜
        gr.Markdown("### FAISS Index Conversion")
        with gr.Row():
            gr.Markdown("Convert news data to FAISS indexes with metadata-based organization")
            
        with gr.Row():
            with gr.Column():
                # ìë™ ë³€í™˜ ì˜µì…˜
                auto_convert_news_data = gr.Checkbox(
                    label="Auto-convert news_data/ folder",
                    value=True
                )
                faiss_use_metadata = gr.Checkbox(
                    label="Use Metadata Extraction",
                    value=True
                )
                faiss_category_filter = gr.Dropdown(
                    label="Filter by Category",
                    choices=["all", "disaster", "crime", "politics", "economics", "society", "culture", "technology", "sports", "entertainment", "health", "environment", "international", "other"],
                    value="disaster"
                )
                faiss_topic_filter = gr.Textbox(
                    label="Filter by Topic",
                    placeholder="e.g., fire, earthquake, infection (leave empty for all)"
                )
                
            with gr.Column():
                # ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ì„ íƒ
                news_data_files = gr.CheckboxGroup(
                    label="Select News Data Files",
                    choices=[],
                    value=[]
                )
                refresh_news_files_btn = gr.Button("ğŸ”„ Refresh News Files", variant="secondary")
                
                # ìˆ˜ë™ ì—…ë¡œë“œ (ì˜µì…˜)
                faiss_upload_button = gr.File(
                    label="Manual Upload (Optional)",
                    file_types=[".json", ".jsonl"],
                    file_count="multiple"
                )
                
                convert_to_faiss_btn = gr.Button("ğŸš€ Convert to FAISS Indexes", variant="primary")
                faiss_output = gr.JSON(label="FAISS Conversion Status")
                faiss_status_output = gr.HTML(label="Conversion Progress")

        # ê²€ìƒ‰ ì„¹ì…˜
        with gr.Row():
            with gr.Column():
                # ì¿¼ë¦¬ ê²€ìƒ‰
                query_input = gr.Textbox(
                    label="Please enter your query",
                    placeholder="Example : What is the cause of the fire in the mixed-use building on December 31, 2023?",
                    elem_classes=["submit-on-enter"],
                    autofocus=True
                )
                
                # ì˜ˆì‹œ ì§ˆë¬¸ (ì§ˆë¬¸ ë°”ë¡œ ì•„ë˜ ë°°ì¹˜)
                gr.Examples(
                    examples=[
                        ["ì§€ë‚œ 3ë…„ê°„ ëŒ€í˜• í™”ì¬ ì‚¬ê±´ë“¤ ì•Œë ¤ì¤˜"],
                        ["í­ì„¤ ì¬í•´ì™€ ê·¸ ì˜í–¥ì— ëŒ€í•´ ì•Œë ¤ì¤˜."],
                        ["ìµœê·¼ ì§€ì§„ ì†Œì‹ê³¼ í”¼í•´ ë³´ê³ ë¥¼ ì•Œë ¤ì¤˜."],
                        ["2021~2022ë…„ ì½”ë¡œë‚˜19 ê°ì—¼ ì¶”ì„¸ë¥¼ ì•Œë ¤ì¤˜."],
                        ["êµí†µì‚¬ê³  í†µê³„ì™€ ì›ì¸ì— ëŒ€í•´ ì•Œë ¤ì¤˜."],
                        ["í­ì—¼ ì¬í•´ì™€ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì•Œë ¤ì¤˜."],
                        ["ì§‘ì¤‘í˜¸ìš° ë° í™ìˆ˜ ë°œìƒ ì‚¬ë¡€ë¥¼ ì•Œë ¤ì¤˜."],
                        ["2021~2022ë…„ ì¬ë‚œ ê´€ë ¨ ë²”ì£„ ì‚¬ê±´ì„ ì•Œë ¤ì¤˜."],
                        ["ë¯¸ì„¸ë¨¼ì§€(PM10) ëŒ€ê¸° ì˜¤ì—¼ê³¼ ê±´ê°• ì˜í–¥ì— ëŒ€í•´ ì•Œë ¤ì¤˜."],
                        ["ì‚°ì‚¬íƒœ ì¬í•´ì™€ ì˜ˆë°© ëŒ€ì±…ì— ëŒ€í•´ ì•Œë ¤ì¤˜."]
                    ],
                    inputs=[query_input],
                    label="ì˜ˆì‹œ ì§ˆë¬¸ (í´ë¦­í•˜ì—¬ ì‚¬ìš©)"
                )

                with gr.Row():
                    # ë‚ ì§œ ë²”ìœ„ í‘œì‹œ
                    date_range_display = gr.Textbox(
                        label="Detected Date Range",
                        interactive=False,
                        placeholder="Date range will be shown here"
                    )

                    # top_k ìŠ¬ë¼ì´ë”
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=50,
                        value=10,
                        step=1,
                        label="ğŸ”¢ Number of Results (Top-K)",
                        info="Select how many search results to retrieve and display"
                    )

                # í•„í„°ë§ ì˜µì…˜
                with gr.Row():
                    gr.Markdown("### Filtering Options")

                with gr.Row():
                    use_date_filter = gr.Checkbox(
                        label="Use Date Filtering",
                        value=True
                    )
                    use_category_filter = gr.Checkbox(
                        label="Use Category Filtering",
                        value=True
                    )
                    use_topic_filter = gr.Checkbox(
                        label="Use Topic Filtering",
                        value=True
                    )

                # ì¹´í…Œê³ ë¦¬ ë° í† í”½ ì„ íƒ
                with gr.Row():
                    category_select = gr.Dropdown(
                        label="Select Category",
                        choices=["all", "politics", "economics", "society", "culture", "technology", "sports", "entertainment", "disaster", "crime", "health", "environment", "international", "other"],
                        value="all"
                    )
                    topic_select = gr.Dropdown(
                        label="Select Topic", 
                        choices=["all"],
                        value="all"
                    )
                    
                # í† í”½ ëª©ë¡ ì—…ë°ì´íŠ¸ ë²„íŠ¼
                update_filters_btn = gr.Button("Update Available Filters", variant="secondary")

                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜µì…˜
                with gr.Row():
                    use_hybrid_search = gr.Checkbox(
                        label="Use Hybrid Search (FAISS + Google)",
                        value=True
                    )
                    generate_comprehensive_answer = gr.Checkbox(
                        label="Generate Comprehensive Answer",
                        value=True
                    )

                # ê²€ìƒ‰ ë²„íŠ¼
                with gr.Column():
                    search_button = gr.Button("Search", variant="primary")
                    hybrid_search_button = gr.Button("Hybrid Search + Answer", variant="primary")

                # ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
                warning_output = gr.HTML()


        # ê²€ìƒ‰ ê²°ê³¼ ì„¤ëª… ì„¹ì…˜
        gr.Markdown("### Explanation of Search Results")
        with gr.Row():
            results_output = gr.Textbox(
                label="News Search Results",
                lines=10,
                show_copy_button=True
            )

        # ì¸ë±ìŠ¤ ê´€ë¦¬ ì„¹ì…˜
        with gr.Row():
            with gr.Column():
                index_info_button = gr.Button("Check Index Status", variant="secondary")
                index_info_output = gr.JSON(label="Index Status")



        # í•¨ìˆ˜ ì •ì˜ë“¤
        def update_index_info():
            return global_manager.load_created_indexes()
        # Enter key submission handler


        def init_model(api_key=None, google_key=None, google_cse=None):
            global global_api_key, global_hybrid_search
            try:
                result = search_interface.init_openai_model(api_key)
                global_api_key = api_key  # OpenAI API í‚¤ ì„¤ì •
                
                # Google API í‚¤ ì—…ë°ì´íŠ¸
                if global_hybrid_search and (google_key or google_cse):
                    global_hybrid_search.google_api_key = google_key
                    global_hybrid_search.google_cse_id = google_cse
                    result += f"\nğŸŒ Google Search API updated: {'âœ…' if google_key and google_cse else 'âš ï¸ Incomplete'}"
                
                return result

            except Exception as e:
                return f"An error occurred during loading: {str(e)}"


        def hybrid_search_with_answer(query, use_date_filter, use_category_filter, use_topic_filter, 
                                     category_select, topic_select, use_hybrid, generate_answer, top_k):
            """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì¢…í•© ë‹µë³€ ìƒì„±"""
            import time
            search_start_time = time.time()
            
            if not global_hybrid_search:
                return "Error: Hybrid search engine not initialized", "No date range available"
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            results, date_range, metadata = return_date_info(query)
            
            try:
                # í•„í„°ë§ ì˜µì…˜ ì¤€ë¹„
                search_category = None
                search_topic = None
                search_date_range = None

                if use_date_filter:
                    search_date_range = date_range

                if use_category_filter:
                    if category_select and category_select != "all":
                        search_category = category_select
                    elif metadata["category"]:
                        search_category = metadata["category"]

                if use_topic_filter:
                    if topic_select and topic_select != "all":
                        search_topic = topic_select
                    elif metadata["topic"]:
                        search_topic = metadata["topic"]

                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ - ëª¨ë“  í•„í„°ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ ê²€ìƒ‰
                if not use_date_filter and not use_category_filter and not use_topic_filter:
                    # ëª¨ë“  í•„í„°ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°: ì „ì²´ ê²€ìƒ‰
                    search_results = global_hybrid_search.search_with_fallback(
                        query=query,
                        k=int(top_k),
                        date_info=None,  # ì „ì²´ ê¸°ê°„
                        category=None,
                        topic=None,
                        use_google=use_hybrid
                    )
                else:
                    # í•„í„°ê°€ í™œì„±í™”ëœ ê²½ìš°: í•„í„° ì ìš© ê²€ìƒ‰
                    search_results = global_hybrid_search.search_with_fallback(
                        query=query,
                        k=int(top_k),
                        date_info=search_date_range,
                        category=search_category,
                        topic=search_topic,
                        use_google=use_hybrid
                    )

                # ë‚ ì§œ í‘œì‹œ ì„¤ì •
                if not use_date_filter and not use_category_filter and not use_topic_filter:
                    date_display = "Hybrid search: All available data (no filters applied)"
                    if use_hybrid:
                        date_display += " [Google Search enabled]"
                else:
                    # ë‚ ì§œ í‘œì‹œ í˜•ì‹ ë³€í™˜
                    start_date, end_date = date_range.split('/')
                    formatted_start = f"{start_date[:4]}-{start_date[4:6]}-{start_date[6:]}"
                    formatted_end = f"{end_date[:4]}-{end_date[4:6]}-{end_date[6:]}"
                    
                    active_filters = []
                    if use_date_filter:
                        active_filters.append("Date")
                    if search_category:
                        active_filters.append(f"Category: {search_category}")
                    if search_topic:
                        active_filters.append(f"Topic: {search_topic}")
                    if use_hybrid:
                        active_filters.append("Google Search")

                    date_display = f"Hybrid search from {formatted_start} to {formatted_end}"
                    if active_filters:
                        date_display += f" [Filters: {', '.join(active_filters)}]"

                # ì¢…í•© ë‹µë³€ ìƒì„± ë˜ëŠ” ê¸°ë³¸ ê²°ê³¼ í‘œì‹œ
                if generate_answer and global_api_key:
                    import openai
                    client = openai.OpenAI(api_key=global_api_key)
                    comprehensive_answer = create_comprehensive_answer(search_results, query, client)
                    results = f"### ğŸ¤– AI ì¢…í•© ë‹µë³€\n\n{comprehensive_answer}\n\n"
                else:
                    results = "### ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼\n\n"
                
                # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í‘œì‹œ (ì ìˆ˜ ë†’ì€ ìˆœì„œëŒ€ë¡œ)
                if not search_results:
                    results += "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.\n"
                else:
                    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì—­ìˆœ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
                    search_results_sorted = sorted(search_results, key=lambda x: x.get('score', 0), reverse=True)
                    
                    results += "### ğŸ“‹ ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ (ì ìˆ˜ ë†’ì€ ìˆœì„œ)\n\n"
                    results += f"**ì´ {len(search_results_sorted)}ê°œ ê²°ê³¼ ë°œê²¬ (ê´€ë ¨ì„± ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)**\n\n"
                    
                    for idx, result in enumerate(search_results_sorted, 1):
                        title = result.get('title', 'No Title')
                        content = result.get('content', '')[:200] + "..."
                        source = result.get('source', 'Unknown')
                        search_type = result.get('search_type', 'unknown')
                        date = result.get('date', '')
                        score = result.get('score', 0)
                        
                        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ì •ë³´ (ìˆëŠ” ê²½ìš°)
                        hybrid_score = result.get('hybrid_score', score)
                        bm25_score = result.get('bm25_score', 0)
                        
                        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                        if len(date) == 8:
                            date = f"{date[:4]}-{date[4:6]}-{date[6:]}"
                        
                        # ê²€ìƒ‰ íƒ€ì…ë³„ ì•„ì´ì½˜
                        type_icon = "ğŸ—ƒï¸" if search_type.startswith("faiss") else "ğŸŒ" if search_type == "google" else "ğŸ“„"
                        
                        # ìˆœìœ„ í‘œì‹œ (1ìœ„ëŠ” ğŸ¥‡, 2ìœ„ëŠ” ğŸ¥ˆ, 3ìœ„ëŠ” ğŸ¥‰)
                        rank_icon = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"#{idx}"
                        
                        results += f"{rank_icon} {type_icon} **{title}**\n"
                        
                        # ì ìˆ˜ ì •ë³´ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ìƒì„¸ í‘œì‹œ)
                        if bm25_score > 0:
                            results += f"ğŸ¯ **Hybrid Score: {hybrid_score:.4f}** (Vector: {score:.3f} + BM25: {bm25_score:.3f})\n"
                        else:
                            results += f"â­ **Score: {score:.4f}**\n"
                        
                        # ì¶œì²˜ ì •ë³´ë¥¼ ë” ëª…í™•íˆ í‘œì‹œ
                        results += f"ğŸ“… {date} | ğŸ“° **ì¶œì²˜: {source}** | ğŸ” **ê²€ìƒ‰ìœ í˜•: {search_type}**\n"
                        results += f"ğŸ“ {content}\n"
                        results += "â”€" * 60 + "\n\n"


                return results, date_display

            except Exception as e:
                return f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "Error in processing"

        def search_with_metadata_filters(query, use_date_filter, use_category_filter, use_topic_filter, category_select, topic_select, top_k):
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            import time
            search_start_time = time.time()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, í† í”½)
            results, date_range, metadata = return_date_info(query)

            if not global_manager:
                return "Error: Manager not initialized", "No date range available"

            # ê²€ìƒ‰ ì‹¤í–‰
            try:
                start_date, end_date = date_range.split('/')
                # ë‚ ì§œ í‘œì‹œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)
                formatted_start = f"{start_date[:4]}-{start_date[4:6]}-{start_date[6:]}"
                formatted_end = f"{end_date[:4]}-{end_date[4:6]}-{end_date[6:]}"

                # í•„í„°ë§ ì˜µì…˜ ì¤€ë¹„
                active_filters = []
                search_category = None
                search_topic = None
                search_date_range = None

                if use_date_filter:
                    active_filters.append("Date")
                    search_date_range = date_range

                if use_category_filter:
                    if category_select and category_select != "all":
                        search_category = category_select
                        active_filters.append(f"Category: {search_category}")
                    elif metadata["category"]:
                        search_category = metadata["category"]
                        active_filters.append(f"Category: {search_category}")

                if use_topic_filter:
                    if topic_select and topic_select != "all":
                        search_topic = topic_select
                        active_filters.append(f"Topic: {search_topic}")
                    elif metadata["topic"]:
                        search_topic = metadata["topic"]
                        active_filters.append(f"Topic: {search_topic}")

                date_display = f"Searching from {formatted_start} to {formatted_end}"
                if active_filters:
                    date_display += f" [Filters: {', '.join(active_filters)}]"

                # ê²€ìƒ‰ ì‹¤í–‰ - ëª¨ë“  í•„í„°ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
                if not use_date_filter and not use_category_filter and not use_topic_filter:
                    # ëª¨ë“  í•„í„°ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°: ë‚ ì§œ í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰
                    search_results = global_manager.search_without_date(
                        query=query,
                        k=int(top_k)
                    )
                    date_display = "Searching all available data (no filters applied)"
                else:
                    # í•„í„°ê°€ í™œì„±í™”ëœ ê²½ìš°: ë©”íƒ€ë°ì´í„° í•„í„° ê²€ìƒ‰
                    search_results = global_manager.search_with_metadata_filters(
                        query=query,
                        k=int(top_k),
                        date_info=search_date_range,
                        category=search_category,
                        topic=search_topic,
                        use_metadata=use_date_filter
                    )

                # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… (ì ìˆ˜ ë†’ì€ ìˆœì„œëŒ€ë¡œ)
                results += "\n### ğŸ” Search Results (ì ìˆ˜ ë†’ì€ ìˆœì„œ)\n"
                if not search_results:
                    results += "No results found with the specified filters.\n"
                else:
                    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì—­ìˆœ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
                    search_results_sorted = sorted(search_results, key=lambda x: x[1], reverse=True)
                    
                    results += f"**ì´ {len(search_results_sorted)}ê°œ ê²°ê³¼ ë°œê²¬ (ê´€ë ¨ì„± ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)**\n\n"
                    
                    for idx, (doc, score) in enumerate(search_results_sorted, 1):
                        doc_date = doc.metadata.get('date', 'Unknown')
                        if len(doc_date) == 8:  # YYYYMMDD í˜•ì‹ì´ë©´ ë³€í™˜
                            doc_date = f"{doc_date[:4]}-{doc_date[4:6]}-{doc_date[6:]}"

                        # ìˆœìœ„ ì•„ì´ì½˜
                        rank_icon = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"#{idx}"
                        
                        results += f"{rank_icon} **{doc.metadata.get('title', 'No Title')}**\n"
                        results += f"ğŸ¯ **Score: {score:.4f}**\n"
                        # ì¶œì²˜ ì •ë³´ë¥¼ ë” ëª…í™•íˆ í‘œì‹œ
                        doc_source = doc.metadata.get('source', 'Unknown')
                        results += f"ğŸ“… {doc_date} | ğŸ“° **ì¶œì²˜: {doc_source}** | ğŸ·ï¸ {doc.metadata.get('category', 'N/A')} | ğŸ”– {doc.metadata.get('topic', 'N/A')}\n"
                        results += f"ğŸ“ {doc.page_content[:200]}...\n"
                        results += "â”€" * 60 + "\n\n"


            except Exception as e:
                results += f"\nError in search: {str(e)}"
                date_display = "Error in date processing"

            return results, date_display

        def update_available_filters():
            """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ì™€ í† í”½ ëª©ë¡ ì—…ë°ì´íŠ¸"""
            if global_manager:
                categories = ["all"] + global_manager.get_available_categories()
                topics = ["all"] + global_manager.get_available_topics()
                return gr.update(choices=categories), gr.update(choices=topics)
            return gr.update(), gr.update()

        def refresh_news_files():
            """ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
            try:
                files = get_news_data_files()
                choices = [file_info for _, file_info in files]
                values = [file_path for file_path, _ in files]
                return gr.update(choices=list(zip(values, choices)), value=[])
            except Exception as e:
                print(f"Error refreshing news files: {e}")
                return gr.update(choices=[], value=[])

        def convert_files_to_faiss(auto_convert, selected_news_files, files, use_metadata, category_filter, topic_filter):
            """FAISS ë³€í™˜ í•¸ë“¤ëŸ¬ (ìë™/ìˆ˜ë™ ëª¨ë“œ ì§€ì›)"""
            if auto_convert:
                # ìë™ ë³€í™˜ ëª¨ë“œ: news_data í´ë” ì‚¬ìš©
                cat_filter = None if category_filter == "all" else category_filter
                top_filter = None if not topic_filter.strip() else topic_filter.strip()
                
                # generatorë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ yield from ì‚¬ìš©
                yield from auto_convert_news_data_to_faiss(
                    selected_files=selected_news_files if selected_news_files else None,
                    use_metadata=use_metadata, 
                    category_filter=cat_filter, 
                    topic_filter=top_filter
                )
            else:
                # ìˆ˜ë™ ì—…ë¡œë“œ ëª¨ë“œ
                if not files:
                    yield None, "Please upload files or enable auto-convert mode."
                    return
                
                cat_filter = None if category_filter == "all" else category_filter
                top_filter = None if not topic_filter.strip() else topic_filter.strip()
                
                # generatorë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ yield from ì‚¬ìš©
                yield from convert_to_faiss_indexes(files, use_metadata, cat_filter, top_filter)


        # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°
        search_button.click(
            fn=search_with_metadata_filters,
            inputs=[query_input, use_date_filter, use_category_filter, use_topic_filter, category_select, topic_select, top_k],
            outputs=[results_output, date_range_display],
            show_progress=True
        )

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°
        hybrid_search_button.click(
            fn=hybrid_search_with_answer,
            inputs=[query_input, use_date_filter, use_category_filter, use_topic_filter, 
                   category_select, topic_select, use_hybrid_search, generate_comprehensive_answer, top_k],
            outputs=[results_output, date_range_display],
            show_progress=True
        )

        # í•„í„° ì—…ë°ì´íŠ¸ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
        update_filters_btn.click(
            fn=update_available_filters,
            outputs=[category_select, topic_select]
        )

        # ë‰´ìŠ¤ íŒŒì¼ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
        refresh_news_files_btn.click(
            fn=refresh_news_files,
            outputs=[news_data_files]
        )

        # FAISS ë³€í™˜ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ (ìë™/ìˆ˜ë™ ëª¨ë“œ ì§€ì›)
        convert_to_faiss_btn.click(
            fn=convert_files_to_faiss,
            inputs=[auto_convert_news_data, news_data_files, faiss_upload_button, 
                   faiss_use_metadata, faiss_category_filter, faiss_topic_filter],
            outputs=[faiss_output, faiss_status_output],
            show_progress=True
        )


        # ëª¨ë¸ ì´ˆê¸°í™” ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°
        init_model_btn.click(
            fn=init_model,
            inputs=[api_key, google_api_key, google_cse_id],
            outputs=[model_status],
        )

        # ì¸ë±ìŠ¤ ê²€ìƒ‰ ì •ë³´
        index_info_button.click(
            update_index_info,
            outputs=[index_info_output]
        )

        # íŒŒì¼ ì—…ë¡œë“œ ë²„íŠ¼ í•¸ë“¤ëŸ¬
        upload_button.change(
            fn=process_uploaded_files,
            inputs=[upload_button, exp_type, question_type],
            outputs=[file_output, status_output],
            show_progress=True
        )

        # í˜ì´ì§€ ë¡œë“œ ì‹œ ë‰´ìŠ¤ íŒŒì¼ ëª©ë¡ ì´ˆê¸°í™”
        demo.load(
            fn=refresh_news_files,
            outputs=[news_data_files]
        )

    return demo

def warmup_models():
    """ëª¨ë¸ ì›Œë°ì—… - ì²« ë²ˆì§¸ ìš”ì²­ ì§€ì—°ì‹œê°„ ê°ì†Œ"""
    global global_retriever, global_retriever_model, global_tokenizer, global_manager

    try:
        print("Warming up models...")

        # ë”ë¯¸ ì¿¼ë¦¬ë¡œ ëª¨ë¸ ì›Œë°ì—…
        dummy_query = "test query for warmup"

        if global_retriever and global_retriever_model and global_tokenizer:
            # ê°„ë‹¨í•œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            if device == 'cuda':
                # GPU ë©”ëª¨ë¦¬ ì›Œë°ì—…
                import time
                start_time = time.time()

                # ì„ë² ë”© ëª¨ë¸ ì›Œë°ì—…
                global_manager.embeddings.embed_query(dummy_query)

                print(f"Model warmup completed in {time.time() - start_time:.2f} seconds")

        print("âœ… Models are warmed up and ready")

    except Exception as e:
        print(f"Warning: Model warmup failed: {e}")

# ...existing code...

if __name__ == "__main__":
    import sys
    import os
    
    # ë³´ì•ˆ ì„¤ì • ì˜µì…˜ (ê¸°ë³¸ê°’: ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©)
    SECURE_MODE = os.getenv('SECURE_MODE', 'false').lower() == 'true'
    AUTH_USERNAME = os.getenv('AUTH_USERNAME', None)
    AUTH_PASSWORD = os.getenv('AUTH_PASSWORD', None)
    
    # ì¸ì¦ ì„¤ì •
    auth_tuple = None
    if AUTH_USERNAME and AUTH_PASSWORD:
        auth_tuple = (AUTH_USERNAME, AUTH_PASSWORD)
        print(f"ğŸ” Authentication enabled for user: {AUTH_USERNAME}")
    
    server_port = int(sys.argv[1]) if len(sys.argv) > 1 else 7870
    
    if SECURE_MODE:
        print("ğŸ”’ Starting in SECURE MODE (localhost only)")
        print(f"   - Server will be accessible only on localhost:{server_port}")
        print(f"   - No external sharing enabled")
        
        demo = create_gradio_interface()
        demo.launch(
            share=False,  # ì™¸ë¶€ ê³µìœ  ë¹„í™œì„±í™”
            server_name="127.0.0.1",  # localhostë§Œ í—ˆìš©
            server_port=server_port,
            debug=False,  # ë³´ì•ˆì„ ìœ„í•´ ë””ë²„ê·¸ ëª¨ë“œ ë¹„í™œì„±í™”
            auth=auth_tuple,  # í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ì¸ì¦ ì‚¬ìš©
            show_error=False  # ì—ëŸ¬ ì •ë³´ ìˆ¨ê¹€
        )
    else:
        # ê¸°ì¡´ê³¼ ë™ì¼í•œ ì„¤ì • (ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©)
        print(f"ğŸŒ Starting server on port {server_port}...")
        print(f"   - External access enabled (same as before)")
        if auth_tuple:
            print(f"   - Authentication enabled for user: {auth_tuple[0]}")
        
        demo = create_gradio_interface()
        demo.launch(
            share=True,  # ê³µìœ  ë§í¬ í™œì„±í™” (ê¸°ì¡´ê³¼ ë™ì¼)
            server_name="0.0.0.0",  # ëª¨ë“  IPì—ì„œ ì ‘ê·¼ í—ˆìš© (ê¸°ì¡´ê³¼ ë™ì¼)
            server_port=server_port,
            debug=True,  # ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ê¸°ì¡´ê³¼ ë™ì¼)
            auth=auth_tuple  # í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ì¸ì¦ ì‚¬ìš©
        )

    # ì‹¤í–‰ ë°©ë²• CUDA_VISIBLE_DEVICES=8 python run.py (7861))