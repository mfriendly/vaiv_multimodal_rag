#!/usr/bin/env python3
"""
Gradio Web Interface for Multimodal RAG Search

Features:
  - Multi-collection search support
  - Text/Image/Hybrid search modes
  - LLM-powered answer generation (OpenAI/Anthropic)
  - Topic filtering
  - Interactive UI with real-time search

Usage:
  python gradio_app.py [PORT]
  
  Default port: 7860
  Example: python gradio_app.py 7870
"""

import gradio as gr
from pathlib import Path
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import from project modules
from pymilvus import MilvusClient

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    from PIL import Image
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    logger.warning("CLIP not available - image search disabled")

import numpy as np
import json

# Import AnswerGenerator from rag_core
from rag_core import AnswerGenerator


class GradioSearcher:
    """Gradioìš© ê°„ì†Œí™”ëœ ê²€ìƒ‰ í´ëž˜ìŠ¤ (run_search.pyì˜ MultimodalSearcher ê¸°ë°˜)"""
    
    def __init__(self, db_file: str, collections: List[str], text_model: str = "jhgan/ko-sroberta-multitask"):
        self.collections = collections if isinstance(collections, list) else [collections]
        self.client = MilvusClient(db_file)
        self.db_file = db_file
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"Using device: {device}")
        
        self.text_embeddings = HuggingFaceEmbeddings(
            model_name=text_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        if CLIP_AVAILABLE:
            self.device = device
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            logger.info("CLIP model loaded for image search")
    
    def search_by_text(self, query: str, top_k: int = 5, topic: str = None) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        filter_expr = f'topic == "{topic}"' if topic and topic != "all" else None
        query_vector = self.text_embeddings.embed_query(query)
        
        all_results = []
        for coll in self.collections:
            if not self.client.has_collection(coll):
                logger.warning(f"Collection '{coll}' not found, skipping")
                continue
            
            results = self.client.search(
                collection_name=coll,
                data=[query_vector],
                limit=top_k,
                filter=filter_expr,
                output_fields=["doc_id", "title", "content", "date", "source", "has_image", "image_path", "category", "topic"]
            )
            
            for hit in (results[0] if results else []):
                hit['_collection'] = coll
                all_results.append(hit)
        
        all_results.sort(key=lambda x: x.get('distance', 0), reverse=True)
        return all_results[:top_k]
    
    def search_by_image(self, image_path: str, top_k: int = 5) -> List[Dict]:
        """ì´ë¯¸ì§€ ê²€ìƒ‰"""
        if not CLIP_AVAILABLE:
            logger.error("CLIP not available for image search")
            return []
        
        image = Image.open(image_path).convert('RGB')
        inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            img_features = self.clip_model.get_image_features(**inputs)
            img_features = img_features / img_features.norm(dim=-1, keepdim=True)
        
        query_vec = img_features.cpu().numpy().flatten().tolist()
        
        all_results = []
        for coll in self.collections:
            if not self.client.has_collection(coll):
                continue
            
            # ì´ë¯¸ì§€ê°€ ìžˆëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰
            results = self.client.search(
                collection_name=coll,
                data=[self.text_embeddings.embed_query("í™”ìž¬")],
                limit=500,
                filter="has_image == true",
                output_fields=["doc_id", "title", "content", "date", "source", "has_image", "image_path", "image_embedding", "category", "topic"]
            )
            
            for r in (results[0] if results else []):
                entity = r.get('entity', {})
                emb_str = entity.get('image_embedding', '[]')
                if not emb_str or emb_str == '[]':
                    continue
                
                stored_vec = json.loads(emb_str)
                sim = np.dot(query_vec, stored_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(stored_vec) + 1e-8)
                r['distance'] = float(sim)
                r['_collection'] = coll
                all_results.append(r)
        
        all_results.sort(key=lambda x: x['distance'], reverse=True)
        return all_results[:top_k]


# Global instances
searcher = None


def init_app(db_file: str, collections: str):
    """ì•± ì´ˆê¸°í™”"""
    global searcher
    
    try:
        colls = [c.strip() for c in collections.split(',')]
        searcher = GradioSearcher(db_file=db_file, collections=colls)
        logger.info(f"âœ… Initialized: {db_file} | Collections: {colls}")
        return f"âœ… Initialized: {db_file}\nðŸ“ Collections: {', '.join(colls)}"
    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        return f"âŒ Initialization failed: {str(e)}"


def format_results(results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    output = []
    for i, r in enumerate(results, 1):
        entity = r.get('entity', {})
        output.append(f"### [{i}] {entity.get('title', 'N/A')}")
        output.append(f"**Score:** {r.get('distance', 0):.4f} | **Collection:** `{r.get('_collection', '')}`")
        output.append(f"ðŸ“… {entity.get('date', 'N/A')} | ðŸ·ï¸ {entity.get('topic', 'N/A')} | ðŸ“° {entity.get('source', 'N/A')}")
        
        content = entity.get('content', '')[:300].replace('\n', ' ')
        output.append(f"\n{content}...\n")
        output.append("---\n")
    
    return "\n".join(output)


def search_fn(query: str, mode: str, image, top_k: int, topic: str, generate_answer: bool, llm: str):
    """ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
    if not searcher:
        return "âš ï¸ ë¨¼ì € DBë¥¼ ì´ˆê¸°í™”í•˜ì„¸ìš” (Settings íƒ­ì—ì„œ Initialize ë²„íŠ¼ í´ë¦­)", ""
    
    results = []
    
    try:
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ì‹¤í–‰
        if mode == "text" and query:
            results = searcher.search_by_text(query, top_k=top_k, topic=topic if topic != "all" else None)
        
        elif mode == "image" and image:
            results = searcher.search_by_image(image, top_k=top_k)
        
        elif mode == "hybrid" and query and image:
            text_results = searcher.search_by_text(query, top_k=top_k*2, topic=topic if topic != "all" else None)
            img_results = searcher.search_by_image(image, top_k=top_k*2)
            
            # ê²°ê³¼ ê²°í•©
            combined = {}
            for r in text_results:
                doc_id = r.get('entity', {}).get('doc_id')
                if doc_id:
                    combined[doc_id] = r
            
            for r in img_results:
                doc_id = r.get('entity', {}).get('doc_id')
                if doc_id and doc_id in combined:
                    combined[doc_id]['distance'] = (combined[doc_id]['distance'] + r['distance']) / 2
                elif doc_id:
                    combined[doc_id] = r
            
            results = sorted(combined.values(), key=lambda x: x['distance'], reverse=True)[:top_k]
        
        else:
            return "âš ï¸ ê²€ìƒ‰ ëª¨ë“œì— ë§žëŠ” ìž…ë ¥ì„ ì œê³µí•˜ì„¸ìš”.", ""
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        formatted = format_results(results)
        
        # ë‹µë³€ ìƒì„± (ì„ íƒì‚¬í•­)
        answer = ""
        if generate_answer and results and query:
            logger.info(f"Generating answer with {llm}...")
            gen = AnswerGenerator(llm=llm)
            answer = gen.generate(query, results)
        
        return formatted, answer
    
    except Exception as e:
        logger.error(f"Search error: {e}")
        import traceback
        traceback.print_exc()
        return f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", ""


def create_interface():
    """Gradio ì¸í„°íŽ˜ì´ìŠ¤ ìƒì„±"""
    with gr.Blocks(title="ðŸ” Multimodal RAG Search", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ðŸ” Multimodal RAG Search System
        
        **Features:**
        - ðŸ“ Text Search: í…ìŠ¤íŠ¸ ì§ˆì˜ë¡œ ê²€ìƒ‰
        - ðŸ–¼ï¸ Image Search: ì´ë¯¸ì§€ë¡œ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê²€ìƒ‰
        - ðŸŽ¯ Hybrid Search: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ê²°í•©í•œ ê²€ìƒ‰
        - ðŸ¤– Answer Generation: LLMì„ í™œìš©í•œ ë‹µë³€ ìƒì„±
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ Settings")
                db_file = gr.Textbox(label="DB File Path", value="./db/fire.db", placeholder="./db/fire.db")
                collections = gr.Textbox(
                    label="Collections (comma-separated)",
                    value="fire_multimodal_demo,disaster_manual",
                    placeholder="fire_news,disaster_manual"
                )
                init_btn = gr.Button("ðŸš€ Initialize", variant="primary")
                init_status = gr.Textbox(label="Status", interactive=False, lines=3)
                
                gr.Markdown("### ðŸ”§ Search Options")
                mode = gr.Radio(["text", "image", "hybrid"], label="Search Mode", value="text")
                top_k = gr.Slider(1, 20, value=5, step=1, label="Top-K Results")
                topic = gr.Dropdown(
                    ["all", "fire", "earthquake", "flood", "typhoon", "heatwave", "coldwave", "chemical", "wildfire", "collapse"],
                    label="Topic Filter",
                    value="all"
                )
                
                gr.Markdown("### ðŸ¤– Answer Generation")
                generate_answer = gr.Checkbox(label="Generate Answer with LLM", value=True)
                llm = gr.Radio(["openai", "anthropic"], label="LLM Provider", value="openai")
                gr.Markdown("*Requires API keys (OPENAI_API_KEY or ANTHROPIC_API_KEY)*")

            with gr.Column(scale=2):
                gr.Markdown("### ðŸ“ Query Input")
                query = gr.Textbox(
                    label="Text Query",
                    placeholder="ì˜ˆ: í™”ìž¬ ë°œìƒ ì‹œ ëŒ€í”¼ ë°©ë²•ì€?",
                    lines=2
                )
                image = gr.Image(label="Image Query (Optional for image/hybrid mode)", type="filepath")
                search_btn = gr.Button("ðŸ” Search", variant="primary", size="lg")
                
                gr.Examples(
                    examples=[
                        ["í™”ìž¬ ë°œìƒ ì‹œ ëŒ€í”¼ ë°©ë²•ì€?"],
                        ["ì§€ì§„ì´ ë°œìƒí•˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"],
                        ["í­ì—¼ ì‹œ ê±´ê°• ê´€ë¦¬ ë°©ë²•"],
                        ["í™”í•™ì‚¬ê³  ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹"],
                        ["ì‚°ë¶ˆì´ ì£¼ê±°ì§€ì—­ì— ì ‘ê·¼í•  ë•Œ ëŒ€ì²˜ë²•"]
                    ],
                    inputs=[query]
                )
                
                gr.Markdown("### ðŸ“Š Search Results")
                results_output = gr.Markdown(label="Search Results")
                
                gr.Markdown("### ðŸ’¬ Generated Answer")
                answer_output = gr.Textbox(label="Answer", lines=10, interactive=False)

        # Event handlers
        init_btn.click(init_app, inputs=[db_file, collections], outputs=[init_status])
        
        search_btn.click(
            search_fn,
            inputs=[query, mode, image, top_k, topic, generate_answer, llm],
            outputs=[results_output, answer_output]
        )
        
        query.submit(
            search_fn,
            inputs=[query, mode, image, top_k, topic, generate_answer, llm],
            outputs=[results_output, answer_output]
        )
    
    return demo


if __name__ == "__main__":
    import sys
    
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 7860
    
    logger.info(f"Starting Gradio app on port {port}...")
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=True
    )
